\section{Error Bounds}
Recall, yet again, the type of functions $f$ we have been talking about. We
assume $f: \Hbb \mapsto \overline{\Rbb}$ is proper, closed, convex and attains
$\min f$. Suppose (wlog) that $\min f = 0$.

\textbf{Desingularizing} function $\varphi: [0, \rho) \mapsto \Rbb_+$,
continuous and concave, $\varphi(0) = 0$, $\varphi' > 0$ continuous on $(0,
\rho)$.

If we know the KL property holds, we know that
\[
	\abs{\grad (\varphi \circ f)}(x) \geq 1, \;
	\forall x \in \cX \supseteq \set{x_k}_{k=1}^{\infty},
\]
where $\set{x_k}$ is a sequence generated by the optimization algorithm used.
Based on this property, we arrive at a framework that enables us to perform
complexity analysis.

Typically, $\cX = \set{x \in \Hbb \mmid 0 < f(x) < \gamma, \; \norm{x - \bar{x}}
\leq \delta}$, with $\bar{x} \in \argmin f$. Additionally, we know that for
certain optimization algorithms, the iterates $\set{x_k}$ will remain in the
region $\cX$ once they first ``hit'' it (e.g. proximal point methods).

Today, we try to address the following $2$ questions:
\begin{itemize}
	\item for a given $f$, does $\varphi$ exist?
	\item how do we go about finding it?
\end{itemize}

The answer the first question: \textit{very often, yes}.

\paragraph{Fact.} Take $\Hbb = \Rbb^n$ and $f$ closed, convex and proper, and
also semialgebraic, i.e. $\epi f$ is a finite union of sets defined by finitely
many polynomial inequalities. Additionally, suppose $\argmin f$ is a compact
set. Then, $\exists \varphi$ such that
\[
	\cX = \set{x \mmid 0 < f(x) < \gamma}. \; \abs{\grad (\varphi \circ f)}{x}
	\geq 1, \; \forall x \in \cX.
\]
Moreover, if we restrict to $\cX \cap r \mathbb{B}$ for some $r > 0$, we can
take $\varphi(s) = k s^{1 - \theta}, \; 0 \leq \theta < 1$.

The above is an existential result. However, it remains to address how we can
find such a function $\varphi$.

\paragraph{How do we find $\varphi$?}
Suppose such a $\varphi$ exists. Fix any $x_0 \in \cX$, and define the
subgradient descent trajectory as
\[
	\begin{cases}
		x(0) = x_0 \\
		\dot{x}(t) \in -\partial f(x(t)), \; \text{a.e. for } t \geq 0.
	\end{cases}
\]
In previous lectures, we showed that $x(\cdot)$ converges strongly to some $x^*
\in \argmin f$ and also stays in $\cX$. Additionally, we proved the uniform
bound
\begin{equation}
	\mathrm{length}(x(\cdot), t, s) \leq
	\varphi(f(x(t))) - \varphi(f(x(s))), \; \forall 0 \leq t < s.
	\label{eq:uniform-bound}
\end{equation}
Letting $t \to 0, s \to \infty$ shows that the length is $\leq \varphi(f(x0))$.
We deduce that
\begin{equation}
	\underbrace{\mathrm{dist}(x, \argmin f) \leq \varphi(f(x))}_{\text{error
	bounds}}, \; \forall x \in \cX.
	\label{eq:error-bound}
\end{equation}
In practice, error bounds seem easier to construct than verifying the KL
property. But, in fact, they are almost equivalent. Let us see why.

In addition to our usual blanket assumptions about $\varphi$, assume that it
grows ``quickly'', i.e. $s \mapsto \frac{\varphi(s)}{s^c}$ is non-decreasing on
$(0, \rho)$. A canonical example is $\varphi(s) = ks^{1 - \theta}, \; c = 1
-\theta$.
With this assumption, we can take the derivative of the ratio and require it to
be nonnegative, i.e.
\begin{align*}
	\varphi'(s) s^c - c s^{c-1} \varphi(s) &\geq 0, \; \forall s > 0
	\Leftrightarrow \frac{s \varphi'(s)}{\varphi(s)} \geq c.
\end{align*}
Assume that~\cref{eq:error-bound} holds. Choose any $x \in \cX$ and take
$\mathrm{proj}_{\argmin}(x) =: y$. Then, since $y \in \argmin f$, we have
\begin{align*}
	\cancelto{0}{f(y)} - f(x) &\geq \ip{\partial^{\circ} f(x), y - x} \Rightarrow
	f(x) \leq \ip{\partial^{\circ} f(x), x - y} \\
	&\leq \norm{\partial^{\circ} f(x)} \norm{x - y} \leq
		\abs{\grad f}(x) \underbrace{\norm{x - y}}_{\text{dist. to argmin}} \\
		&= \abs{\grad f}(x) \mathrm{dist}(x, \argmin f)
\end{align*}
Now, using our error bound and the chain rule shown in a previous lecture, we
obtain that
\begin{align*}
	f(x) &\leq \abs{\grad f}(x) \varphi(f(x)) \leq
		\abs{\grad f}(x) \frac{f(x) \varphi'(f(x))}{c} \\
	\Rightarrow \abs{\grad\left(\frac{1}{c} \varphi \circ f\right)}(x) &\geq 1.
\end{align*}
Observe that the last inequality is precisely the KL property with
desingularizer $\frac{1}{c} \varphi$.

\paragraph{Example: alt. projections.} Take $C, D$ to be closed, convex sets.
Assume that $C, D$ intersect in a reasonable way, i.e. $\delta \mathbb{B}
\subseteq C \cap D$, for simplicity. Take any $x \in \Hbb$ and write
\[
	u = \mathrm{proj}_C(x) \Rightarrow \norm{u} \leq \norm{x}, \;
	\text{since } 0 \in C.
\]
Now, take $v = \mathrm{proj}_D(x)$. Suppose that $\mathrm{dist}(x, C),
\mathrm{dist}(x, D) \leq \frac{d}{2}$ for some parameter $d$, which we can
compute if we assume that we can compute the projections. Then, this implies
\begin{align*}
	\norm{u - v} &= \norm{u - x + x - v} \leq \norm{u - x} + \norm{v - x} \\
		&\leq \mathrm{dist}(x, C) + \mathrm{dist}(x, D) \leq d.
\end{align*}
Hence we conclude that $\frac{\delta}{d} (u - v) \in C \cap D$, since its norm
is at most $\delta$. Take
\[
	z = \frac{\delta}{\delta + d} u = \frac{\delta}{\delta + d} u +
	\frac{d}{\delta + d} 0 \in C, \text{ by convexity.}
\]
But also,
\[
	z = \frac{d}{\delta + d} \underbrace{\frac{\delta}{d} (u - v)}_{\in D}
	+ \frac{\delta}{\delta + d} v \in D, \text{ again by convexity.}
\]
This implies that $z \in C \cap D$, so we obtain
\begin{align*}
	\mathrm{dist}(x, C \cap D) &\leq \mathrm{dist}(x, z) =
		\norm{x - \frac{\delta}{\delta + d} u} \\
		&\leq \norm{x - u} + \norm{u - z} = \frac{d}{2}
			+ \frac{\delta}{\delta + d} \norm{u} \\
		&\leq \frac{d}{2} \left(1 + \frac{2\norm{x}}{\delta + d}\right),
\end{align*}
since $\norm{u} \leq \norm{x}$ for the last inequality. We conclude that
\[
	\mathrm{dist}(x, C \cap D) \leq \left( 1 + \frac{2\norm{x}}{\delta}\right)
	\max\set{\mathrm{dist}(x, C) + \mathrm{dist}(x, D)}.
\]
Notice that alternating projections is simply proximal gradient applied on
\[
	f(x) = \delta_C(x) + \frac{1}{2} \mathrm{dist}^2(x, D).
\]
However, we have our error bound $\mathrm{dist}(x, C \cap D)
\leq \left(1 + \frac{2\norm{x_0}}{\delta} \right) \sqrt{2 f(x)}$, since
$\set{x_k} \in C$. Hence KL property holds, allowing us to deduce a complexity
bound for alternating projections.
