\section{Proximal Gradient for LASSO}
Continuing on our discussion about error bounds, today we present another
example based on a famous problem.

Recall the $\ell_1$-regularized least squares problem:
\begin{equation}
	\min_{x \in \Rbb^n} \frac{1}{2} \norm{Ax - b}_2^2 + \mu \norm{x}_1
	=: f(x).
	\label{eq:lasso-problem}
\end{equation}
Notice that $\forall \bar{x} \in \argmin f$:
\[
	\mu \norm{\bar{x}}_1 \leq f(\bar{x})
	= \min f \leq \frac{\norm{b}^2_2}{2} \Rightarrow
	\norm{\bar{x}}_1 \leq \frac{\norm{b}_2^2}{2 \mu}.
\]
We want to convert~\cref{eq:lasso-problem} to an equivalent problem that is
amenable to analysis. Consider the equivalent problem:
\begin{align}
	\begin{aligned}
	\mbox{Minimize } & \frac{1}{2} \norm{Ax - b}_2^2 + y \\
	\mbox{s.t. } & \norm{x}_1 \leq y \\
				 & y \leq \frac{\norm{b}^2}{2\mu}
	\end{aligned}
	\label{eq:lasso-equiv}
\end{align}
\cref{eq:lasso-equiv} has the form
\[
	\min_{x} \frac{1}{2} \norm{Cz - d}_2^2 + \ip{e, z}, \;
	z = \begin{pmatrix} x \\ y \end{pmatrix}, \;
	z \in \text{polytope } K.
\]
We now derive an error bound for $g(z) := \frac{1}{2} \norm{Cz - d}_2^2 +
\ip{e, z}$, of the form
\begin{equation}
	\mathrm{dist}(z, \argmin_K g) \leq
	\nu^2 \lambda \cdot \left( g(z) - \min_{K} g \right).
	\label{eq:error-bound-lasso}
\end{equation}
In the above, $\nu$ is a \textbf{Hoffman constant}. This comes from the fact
that
\begin{equation}
	\mathrm{dist}\left(z, \begin{pmatrix} C \\ e \end{pmatrix}^{-1} w \right)
	\leq \nu \norm{\begin{pmatrix} C \\ e \end{pmatrix} z - w}_2, \;
	\forall z \in K.
\end{equation}
The existence of $\nu$ is guaranteed by a Theorem due to (Hoffman, 1952).

\paragraph{A few words about Hoffman's Theorem.} Suppose we are solving a
system of linear inequalities $Gx \leq p$, where $G \in \Rbb^{m \times n}$ is a
fixed matrix. When this system is feasible, that problem is equivalent to
minimizing
\(
	\norm{(Gx - p)_+}_2,
\)
where the notation $(x)_+$ denotes the componentwise positive part of $x$.
\begin{ctheorem}{Hoffman's Theorem}{hoffman}
	There $\exists \nu$ such that
	\[
		\mathrm{dist}\left(x, \set{u \mmid Gu \leq p}\right)
		\leq \nu \norm{(Gx - p)_+}_2.
	\]
\end{ctheorem}
If we define $\Phi$ to be a set valued mapping by $\Phi(x) = Gx + \Rbb_+^n$,
the theorem above says that
\[
	\mathrm{dist}\left(x, \Phi^{-1}(p)\right) \leq \nu \mathrm{dist}(p,
	\Phi(x)).
\]
The above is a ``global'' form of \textbf{metric regularity}, since it does not
hold only locally. In fact, this is true for any polyhedral set-valued mapping
$\Phi$.

Now, back to our original problem. In~\cref{eq:error-bound-lasso}, we have that
\begin{align*}
	\lambda &= \norm{e} D + 3 G D' + 2 G^2 + 2 \\
	D &= \text{diameter of } K \\
	D' &= \text{diameter of } \begin{pmatrix} C \\ e \end{pmatrix} K \\
	G &= \max_{z \in K} \norm{C^\top (Cz - d)},
\end{align*}
so we see that the error bound is concrete in terms of initial data. It now
simply remains to translate that into an error bound for the original problem:
\[
	\mathrm{dist}(x, \argmin f) \leq
	\kappa \sqrt{f(x) - \min f}, \; \forall x,
\]
where $\kappa$ is a universal constant as long as we restrict $x$ into a
compact set $\norm{x} \leq R$, similar to the one we identified earlier.

We now employ what we know about the relation between error bounds and the KL
inequality, with desingularizing function $\phi(s) = \kappa' \sqrt{s}$.
Finally, we deduce that ISTA (proximal gradient for LASSO) converges linearly
because of the linear convergence of the proximal point method for the function
$\psi(t) = c t^2$, at an explicit rate.

\section{Alternating Minimization}
We will outline a brief roadmap of what will be covered in the next few weeks.

\subsubsection{Gauss-Seidel}
\paragraph{Example: alternating projections for closed, convex $C, D$}.
\begin{algorithm}
	\caption{Alternating projections}
	\begin{algorithmic}
		\Repeat
			\State $x \gets \mathrm{proj}_C(y) $
			\State $y \gets \mathrm{proj}_D(x) $
		\Until{some stopping criterion}
	\end{algorithmic}
	\label{alg:alternating-projections}
\end{algorithm}
We could model the above simply as solving
\[
	\min_{x, y} \set{ \delta_C(x) +
		\frac{1}{2} \norm{x - y}_2^2
	\delta_D(y) }
\]
by alternately minimizing over $x$, with $y$ fixed, and then over $y$ with $x$
fixed.

\textbf{Caveat}: cyclic coordinate minimization, even if the functions involved
are smooth, may not converge to even a critical point.

\paragraph{Example: (Powell, 72).}
Define $\phi(x, y, z) = (x - 1)^2_+ + (-x - 1)^2_+ - yz$. Then form the
symmetric function
\[
	f(x, y, z) = \phi(x, y, z) + \phi(z, x, y) + \phi(y, z, x).
\]
The function $f$ is nonconvex, but $f \in C^1$. Starting just outside the unit
cube, near a non-optimal vertex, cyclic coordinate descent will converge to a
cycle around the $6$ non-critical vertices.
In general, even in the convex and smooth case, convergence can be a delicate
and difficult matter.

In the sequel, we will consider the following model:
\[
	\min_{x, y} \set{\underbrace{f(x) + g(y) + H(x, y)}_{\Psi(x, y)}}
\]
where $H(x,y)$ is a so-called \textit{coupling function}, and $x, y$ can be
blocks of variables. Our typical assumption will be that $H \in C^2$, and that
$f, g$ are ``\textit{prox-friendly}'', but nonsmooth.

\begin{algorithm}
	\caption{Alternating Proximal Gradient}
	\begin{algorithmic}
		\Repeat
			\State $x \gets \prox{\lambda f}{x - \lambda \grad_x H(x, y)}$
			\State $y \gets \prox{\mu g}{y - \mu \grad_y H(x, y)}$
		\Until{some stopping criterion.}
	\end{algorithmic}
	\label{alg:alt-prox-grad}
\end{algorithm}

Our main algorithm will be~\cref{alg:alt-prox-grad}. We will prove that bounded
sequences generated by it must converge to critical points, provided that the
function $\Psi(x, y)$ satisfies the KL property (e.g. if $\Psi$ is
semialgebraic). In particular, we will \textbf{not} assume any convexity.

\paragraph{Example: alternating projections (again)}. For the particular case
where $f = \delta_C$, $g = \delta_D$ and $H(x, y) = \frac{1}{2} \norm{x -
y}^2$, we get the following sequence of updates based
on~\cref{alg:alt-prox-grad}:
\begin{algorithm}
	\caption{Relaxed Alternating Projections}
	\begin{algorithmic}
		\Repeat
			\State $x \gets (1 - \delta) \mathrm{proj}_C(y) + \delta y$
			\State $y \gets (1 - \delta) \mathrm{proj}_D(x) + \delta x$
		\Until{some stopping criterion}
	\end{algorithmic}
\end{algorithm}
%TODO : Check if the above is correct after plugging into the equation

