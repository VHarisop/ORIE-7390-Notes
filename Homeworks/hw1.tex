\documentclass[10pt]{article}

\input{preamble}
\usepackage{latex-macros}
\usepackage{todonotes}
\usepackage{tikz,pgfplots}
\usepackage{algorithm, algpseudocode}
\usepackage[margin=1in]{geometry}
\usetikzlibrary{shapes.geometric}
%\usepackage{parskip}
\usepackage[capitalize]{cleveref}
\usepackage{exercise}

\usepackage{todonotes}

\newcommand{\regdiff}{\hat{\partial}}
\newcommand{\bd}[1]{\mathrm{bd}\left( #1 \right)}

\begin{document}

\allowdisplaybreaks
\everymath{\displaystyle}

\homework{Vasileios Charisopoulos}{Homework 1}

\begin{Exercise}
	Consider $f(u, v) = \abs{u} + v^2$, so $\dom f = \Rbb^2$.

	\ExePart

		Let us simplify the notation by dropping $\lambda$ from $x_k^{\lambda}$
		for now, until we determine the solution of the proximal map.
		Consider $x^* = \argmin_{x} f(x) + \frac{\lambda}{2} \norm{x - x_k}^2$.
		Since $f$ is a convex function, the proximal map is well defined and,
		additionally, based on the $f$ having full domain, we can conclude that
		\[
			\partial \left(f(x) + \frac{\lambda}{2} \norm{x - x_k}^2 \right)
			= \partial f(x) + \partial \left( \frac{\lambda}{2} \norm{x -
			x_k}^2 \right).
		\]
		Therefore we simply have to write the first-order optimality conditions
		for $x^*$, which means that
		\begin{align*}
			0 &\in \partial f(x^*) + \lambda ( x^* - x_k ) \Rightarrow
			0 \in \begin{pmatrix} \partial \abs{x^*_1} \\ 2 x^*_2 \end{pmatrix}
				+ \lambda \begin{pmatrix} x^*_1 - x_k^{(1)} \\ x^*_2 - x_k^{(2)}
				\end{pmatrix},
		\end{align*}
		which we arrived at observing that the function minimized is separable.
		For the smooth part, trivial algebraic manipulations lead to $x_2^* =
		\frac{\lambda}{2 + \lambda} x_k^{(2)}$. For the nonsmooth part, we
		know from ORIE 6328 that the solution is given by the
		\textit{soft thresholding} operator. Nevertheless, we repeat the
		derivation to convince the reader. Consider the following cases:
		\begin{enumerate}
			\item $x^*_1 > 0$: in that case $\partial \abs{x_1}^* = 1$ and
				$x_k^{(1)} = x_1 + \frac{1}{\lambda}$.
			\item $x^*_1 < 0$: like before, we have $\partial \abs{x_1}^* = -1$
				leading to $x_k^{(1)} = x_1 - \frac{1}{\lambda}$.
		\end{enumerate}
		The two cases above imply that $\sign(x_k^{(1)}) = \sign(x_1^*)$ when
		$x_1^* \neq 0$. Now, consider the case where it is equal to $0$. We
		know from convex analysis that $\partial \abs{x} = [-1, 1]$ when
		$x = 0$, so $x_k^{(1)}$ must be $\in \frac{[-1, 1]}{\lambda}$. Gathering all the
		cases above (and keeping in mind that $\lambda > 0$) gives us
		\begin{align*}
			x_1^* &= \sign(x_k^{(1)}) \max\left(\abs{x_k^{(1)}} -
				\frac{1}{\lambda}, 0\right).
		\end{align*}
		Therefore, we have
		\[
			x_{k+1}^{\lambda} = \begin{pmatrix}
				\sign(x_k^{(1)}) \max\left( \abs{x_k^{(1)}} -
				\frac{1}{\lambda}, 0 \right) \\
				\frac{\lambda}{2 + \lambda} x_k^{(2)}
			\end{pmatrix}, \; \forall \lambda > 0.
		\]
\end{Exercise}

\begin{Exercise}
	Consider $f: \Rbb^n \to \Rbb$, continuously differentiable. This means that
	$f$ satisfies the following at any point $x$:
	\begin{align}
		f(x + d) &= f(x) + \ip{\grad f(x), d} + o(d), \;
		\lim_{d \to 0} \frac{o(d)}{\norm{d}} = 0.
		\label{eq:diff-defn}
	\end{align}
	First, consider $x$ being a minimizer. Then, we have defined the slope to
	be $0$ by convention, which agrees with the first-order condition $\grad
	f(x) = 0$ in unconstrained optimization. The nontrivial case has $x$ not
	being a minimizer.

	Let us work with the definition of the slope. We write
	\begin{align*}
		\abs{\grad f}(x) &= \limsup_{z \to x} \frac{f(x) - f(z)}{\norm{x - z}}
		= \lim_{\delta \dto 0} \sup_{\norm{z - x} \leq \delta} \frac{f(x) -
		f(z)}{\norm{x - z}} \\
			&= \lim_{\delta \dto 0} \sup_{\norm{d} \leq \delta}
			\frac{f(x) - f(x + d)}{\norm{d}} =
				\lim_{\delta \dto 0} \sup_{\norm{d} \leq \delta}
			\frac{f(x) - f(x) - \ip{\grad f(x), d} + o(d)}{\norm{d}} \\
			&\overset{(\text{Cauchy-Schwarz})}{\leq} \limsup_{d \to 0} \left(
				\frac{\norm{\grad f(x)} \norm{d}}{\norm{d}} +
			  	\frac{o(d)}{\norm{d}} \right) = \norm{\grad f(x)},
	\end{align*}
	where we used the fact that $\frac{o(d)}{\norm{d}} = 0$ as $d \to 0$.
	It is left to show that $\abs{\grad f}(x) \geq \norm{\grad f(x)}$, and the
	proof will be complete. To that end, observe that since $z \to x$ in the
	limit superior of $\abs{\grad f}$'s definition, we must have for any
	$d \in \ball_2$:
	\[
		\abs{\grad f}(x) \geq \lim_{t \to 0} \frac{f(x) - f(x + td)}{t},
	\]
	which is simply the mathematical way to state the fact that approaching $x$
	by arbitrary trajectories can only give us a bigger $\sup$ than by
	approaching it from a single direction. Then, setting $d = -\frac{\grad
	f(x)}{\norm{\grad f(x)}}$ and replacing $f(x + td)$ by~\cref{eq:diff-defn}
	gives us:
	\begin{align*}
		\abs{\grad f}(x) &\geq
		\lim_{t \dto 0} \frac{f(x) - f(x) + t \norm{\grad f(x)} + o(t)}{t}
			= \norm{\grad f(x)},
	\end{align*}
	which completes the claim. Hence $\abs{\grad f}(x) = \norm{\grad f(x)}$
	when $f$ is $C^1$.
\end{Exercise}

\begin{Exercise}

	\ExePart

	Consider the map $t \mapsto \frac{f(x + tv) - f(x)}{t}$, and let us set
	$g(t) = f(x + tv) - f(x)$. This map satisfies $g(0) = 0$ and is convex since
	\begin{align*}
		g(\lambda t_1 + (1 - \lambda) t_2) &=
		f\left(x + (\lambda t_1 + (1 - \lambda) t_2) v \right) - f(x) \\
		&= f(\lambda (x + t_1 v) + (1 - \lambda) (x + t_2 v)) - f(x)
		\leq \lambda f(x + t_1 v) + (1 - \lambda) f(x + t_2 v)
			- \lambda f(x) - (1 - \lambda) f(x) \\
		&= \lambda g(t_1) + (1 - \lambda) g(t_2),
	\end{align*}
	where in the above we only made use of the convexity of $f$ with respect to
	its argument. The rest is trivial: consider $t_2 \leq t_1$, which means that
	we can write $t_2 = \lambda t_1, \; 0 \leq \lambda \leq 1$, so that
	\begin{align*}
		g(t_2) &= g(\lambda t_1 + (1 - \lambda) 0) \leq
			\lambda g(t_1) + (1 - \lambda) \cancelto{0}{g(0)} \Rightarrow \\
			g(t_2) &\leq \lambda g(t_1)
	\end{align*}
	It is trivial to verify that $\lambda \in \set{0, 1}$ gives us trivial
	implications. Then, for $\lambda \in (0, 1)$, we rewrite $\lambda =
	\frac{t_2}{t_1}$ by definition, so we obtain
	\[
		\frac{g(t_2)}{t_2} \leq \frac{g(t_1)}{t_1}, \; t_1 \geq t_2
	\]
	which proves that $\frac{g(t)}{t} = \frac{f(x + tv) - f(x)}{t}$ is
	non-decreasing.

	\ExePart

	Here, we only know that $x \in \dom f$, which means that there could exist
	directions $v \in \Rbb^n$ such that $f(x + tv) \notin \dom f \Rightarrow
	f(x + tv) = \infty$. Additionally, since we can only guarantee local
	(Lipschitz) continuity when $x \in \intr{\dom f}$, there could be cases
	where $\abs{f(x + tv) - f(x)} > \epsilon, \; \forall t > 0$. We will
	consider the following mutually exclusive cases:
	\begin{itemize}
		\item $x + tv \notin \dom f, \; \forall t > 0$, which means that $v$
			extends into infeasibility in constraint scenarios.
		\item $x + tv \in \dom f, \forall t$ small enough, but $f(x + tv) -
			f(x) < -\epsilon$, where $\epsilon > 0$ is fixed.
			This means that $f$ is discontinuous at $x$.
		\item $x \in \intr{\dom f}$, which automatically means that $f$ is
			continuous around $x$.
	\end{itemize}
	Note that one case we omitted is when $f(x + tv) - f(x) > \epsilon, \;
	\forall t$ small enough. This case is straightforward to deduce as
	impossible: the algebraic definition of convexity for $f$ will give us
	\begin{align*}
		f(x) + \epsilon &\leq f(x + tv) = f(t(x + v) + (1 - t)(x + 0)) \\
			&\leq t f(x + v) + (1 - t) f(x) \Rightarrow ( t \dto 0 ) \\
		f(x) + \epsilon &\leq \lim_{t \dto 0} t f(x + v) + (1 - t) f(x)
		\Rightarrow \epsilon \leq 0,
	\end{align*}
	which is a contradiction.

	Let us first try to deduce that $g(v) \tdef f'(x; v)$ is positively
	homogeneous. We need to check two postulates:
	\begin{enumerate}
		\item $g(0) = 0$: this is trivial since
			\[
				\lim_{t \dto 0} \frac{f(x + t 0) - f(x)}{t} =
				\lim_{t \dto 0} \frac{f(x) - f(x)}{t} = 0.
			\]
		\item $g(cv) = c g(v)$ for $c > 0$: we will proceed on a case-by-case basis. If $x
			+ tv$ extends outside $\dom f$ for all small $t > 0$, we write
			\begin{align*}
				g(cv) &= \lim_{t \dto 0} \frac{f(x + c t v) - f(x)}{t} \\
					  &= \lim_{t \dto 0} \frac{\infty - f(x)}{t} = \infty,
			\end{align*}
			since $c t > 0$ as well. Additionally, $g(v) = \infty$ by the same
			reasoning, since $f(x + tv) = \infty, \; \forall t > 0$. Hence
			$\infty = g(cv) = c g(v) = \infty$ as $c > 0$.

			If $x, v$ are such that $f(x + tv) - f(x) < -\epsilon$ for small
			$t$, then we obtain $\lim_{t \dto 0} \frac{f(x + tv) - f(x)}{t} =
			-\infty$ and the same line of reasoning will give us
			\( g(cv) = -\infty = c \cdot (-\infty) \).

			Finally, if $x \in \intr{\dom f}$, we showed in class that
			$f(x; v) = \max_{y \in \partial f(x)} \ip{y, v}$ and then it is
			trivial to verify that
			\[
				g(cv) = \max_{y \in \partial f(x)} \ip{y, cv} =
					\max_{y \in \partial f(x)} c \ip{y, v} = c g(v)
			\]
			by the linearity of the inner product.
	\end{enumerate}
	Since we have imposed no constraints on the choice of $v$, we can conclude
	that $g(v)$ must be positively homogeneous.

\end{Exercise}
\end{document}
