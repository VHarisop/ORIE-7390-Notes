\documentclass[10pt]{article}

\input{preamble}
\usepackage{latex-macros}
\usepackage{todonotes}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{algorithm, algpseudocode}
\usepackage[margin=1in]{geometry}
\usetikzlibrary{shapes.geometric}
\usepackage[capitalize]{cleveref}
\usepackage{exercise}
\usepackage{svg}
\setminted{
    linenos=true,
    autogobble,
}
\usepackage{todonotes}

\newcommand{\regdiff}{\hat{\partial}}
\newcommand{\bd}[1]{\mathrm{bd}\left( #1 \right)}
\newcommand{\eps}{\varepsilon}

\begin{document}

\allowdisplaybreaks
\newenvironment{longlisting}{\captionsetup{type=listing}}{}

\homework{Vasileios Charisopoulos}{Homework 3}{vc333}

\begin{Exercise}
	Consider a closed, proper, convex function $f: \Rbb^n \to \Rbb$. If a
	sequence $\set{(x^k, y^k)}$ converges in the graph of the subdifferential
	$\partial f$, prove that the function values also converge. Are all the
	assumptions about $f$ necessary?
\end{Exercise}
\begin{Answer}
	Convergence in the graph of the subdifferential implies that $\exists
	\set{x^k} \to \xbar$ and $\set{y^k} \to g \in \partial f(\xbar)$ such that
	$y^k \in \partial f(x^k)$. Notice that $\partial f(\xbar) \neq \emptyset$
	since the $\dom f = \Rbb^n$. We now assume that the function values do not
	converge, i.e. $\exists \eps > 0$ such that $\abs{f(x^k) - f(\xbar)} \geq
	\eps$, for all $k \in \Nbb$. Notice that $f$ is assumed to be closed, so
	lower semicontinuous, which implies that $\forall \delta > 0$, there exists
	a ball of radius $\rho > 0$ with $f(x) > f(\xbar) - \delta, \; \forall x \in
	\xbar + \rho \mathbb{B}$. Unfolding the divergence hypothesis means that
	\[
		\abs{f(x^k) - f(\xbar)} \geq \eps \Rightarrow
		\begin{cases}
			f(x^k) \geq f(\xbar) + \eps, & \text{ or } \\
			f(x^k) \leq f(\xbar) - \eps
		\end{cases},
	\]
	but appealing to lower semicontinuity of $f$ with $\delta := \eps$ makes
	the second of the above possibilities not applicable. It remains to show
	that $f(x^k) \geq f(\xbar) + \eps$ is impossible. Notice the following
	chain of inequalities:
	\begin{align*}
		f(x^k) &\geq f(\xbar) + \eps \geq f(x^k) + \ip{y^k, \xbar - x^k} + \eps
			    \geq f(x^k) - \norm{y^k}_2 \norm{\xbar - x^k}_2 + \eps,
	\end{align*}
	where the penultimate inequality is the subgradient inequality with $y^k \in
	\partial f(x^k)$ and the last inequality follows from Cauchy-Schwarz. Now,
	convergence in the subdifferential graph implies that
	\begin{itemize}
		\item $\forall \eps_1, \; \exists K_1$ such that $\norm{x^k - \xbar}
			\leq \eps_1, \; \forall k \geq K_1$, and
		\item $\forall \eps_2, \; \exists K_2$ such that $\norm{y^k - g}
			\leq \eps_2, \; \forall k \geq K_2$, where $g \in \partial
			f(\xbar)$.
	\end{itemize}
	Finally, an appeal to local Lipschitzness of $f$, which follows since $f$ is
	convex, implies that $\exists \eta > 0$ such that $\norm{g} \leq L$ for
	some $L > 0$, which is identified by the local Lipschitz modulus.
	Therefore:
	\[
		\norm{y^k} \leq \norm{y^k - g} + \norm{g} \leq \norm{y^k - g} + L,
	\]
	and we can set $\eps_2 := \kappa, \; \eps_1 := \frac{\eps}{2 (\kappa + L)}$,
	so that for large enough $k$, we have
	\begin{align*}
		\norm{y^k} \norm{x^k - \xbar} &\leq \eps_1 \eps_2 \leq
		\frac{\eps}{2 (\kappa + L)} \cdot (\kappa + L) \leq \frac{\eps}{2} \\
		\Rightarrow f(x^k) \geq f(x^k) - \frac{\eps}{2} + \eps = f(x^k) +
		\frac{\eps}{2},
	\end{align*}
	which is a contradiction since $\eps > 0$. Therefore, $f$ must converge in
	function value. Notice that we have used the fact that $f$ is proper
	implicitly in the sense that our $f(x^k)$ are finite.

	We could follow the same reasoning without explicitly requiring $f$ to be
	closed; since $\dom f = \Rbb^n$, our point $\xbar \in \intr{\dom f}$, otherwise an
	``approaching'' sequence $\set{x_k}$ must, in fact, diverge. This
	immediately implies that $f$ is lower semicontinuous around $\xbar$ by
	virtue of being locally Lipschitz continuous. The assumption that $f$ is
	convex might also be unnecessary; as mentioned in class, continuous
	functions are  subdifferentially regular without other explicit assumptions.
\end{Answer}

\begin{Exercise}
	Consider a closed proper function $f: \Rbb^n \to \Rbb$ with the property
	that the function $f - \norm{\cdot}^2$ is convex. Prove that $f$ has a
	unique minimizer.
\end{Exercise}
\begin{Answer}
	We first give an elementary proof of the fact that $f$ is strongly convex.
	Applying the algebraic criterion of convexity to $f - \norm{\cdot}^2$ gives
	us, for $\lambda \in (0, 1), \; x \neq y$:
	\begin{align*}
		f(\lambda x + (1 - \lambda) y) - \norm{\lambda x + (1 - \lambda) y}^2
		&\leq \lambda f(x) + \lambda \norm{x}^2 + (1 - \lambda) f(y) + (1 -
		\lambda) \norm{y}^2 \nonumber \\
		f(\lambda x + (1 - \lambda) y) - \lambda^2 \norm{x}^2 + (1 - \lambda)^2
		\norm{y}^2 - 2 \lambda(1 - \lambda) \ip{x, y} &\leq
			\lambda f(x) + (1 - \lambda) f(y) - \lambda \norm{x}^2 - (1 -
			\lambda) \norm{y}^2,
	\end{align*}
	so we arrive at
	\begin{align}
		f(\lambda x + (1 - \lambda) y) &\leq
		\lambda f(x) + (1 - \lambda) f(y) - (\lambda - \lambda^2) \norm{x}^2
		- \left( (1 - \lambda) - (1 - \lambda)^2 \right) \norm{y}^2
		+ 2 \lambda (1 - \lambda) \ip{x, y}.
		\label{eq:f-ineq}
	\end{align}
	Now, notice that we can simplify $(1 - \lambda) - (1 - \lambda)^2 =
	(1 - \lambda)( 1 - (1 - \lambda)) = \lambda(1 - \lambda)$ and also $\lambda
	- \lambda^2 = \lambda(1 - \lambda)$, so that
	\begin{align*}
		-(\lambda - \lambda^2) \norm{x}^2 - \left( (1 - \lambda) - (1 -
		\lambda)^2 \right) \norm{y}^2 + 2 \lambda(1 - \lambda) \ip{x, y} &=
		-\lambda (1 - \lambda) \left(\norm{x}^2 + \norm{y}^2 - 2 \ip{x,
		y}\right) \\
			&= -\lambda(1 - \lambda) \norm{x - y}^2,
	\end{align*}
	which is strictly negative when $x \neq y$ and $\lambda \in (0, 1)$. This
	implies the following, when combined with~\cref{eq:f-ineq}:
	\[
		f(\lambda x + (1 - \lambda) y) < \lambda f(x) +
		(1 - \lambda) f(y), \; \forall x \neq y, \; \lambda \in (0, 1).
	\]
	Now, we will prove that $f$ has \textit{at most} one minimizer. Suppose
	that $\exists x_1 \neq x_2$ such that $f(x_1) = f(x_2) \leq f(z), \;
	\forall z \in \Rbb^n$ (it is clear that minimizers have the same function
	value due to the fact that $f$ is convex).
	Then, for some $0 < t < 1$, set $z := t x_1 + (1 - t) x_2$, for which we
	know:
	\begin{align*}
		f(t x_1 + (1 - t) x_2) & < t f(x_1) + (1 - t) f(x_2) = t f(x_1) + (1 - t)
		f(x_1) = f(x_1) \Rightarrow f(z) < f(x_1),
	\end{align*}
	a clear contradiction to our assumption. Therefore, $f$ has \textbf{at
	most} one minimizer.

	It remains to prove that $f$ attains a minimizer. Assume, on the contrary,
	that $f$ does not attain a minimizer. Given the convexity of $f -
	\norm{\cdot}^2$, we have that
	\begin{align*}
		f(x) - \norm{x}^2 &\geq f(0) + \ip{\partial \left(f - \norm{\cdot}^2
		\right)(0), x - 0} = f(0) + \ip{\partial f(0), x} - 2 \ip{0, x} =
		f(0) + \ip{\partial f(0), x},
	\end{align*}
	where the existence of $\partial g$ follows from the fact that its domain
	is the whole underlying space and we use the sum rule of subdifferential
	calculus for the sum of a convex ($f$) and a differentiable
	($-\norm{\cdot}^2$) function\footnote{In fact, in ORIE 6328, we proved this
	rule for the regular subdifferential. Since $f - \norm{\cdot}^2$ is convex,
    the regular subdifferential coincides with the convex subdifferential}.
    This means that
	\[
		f(x) \geq \norm{x}^2 + \ip{\partial f(0), x}, \; \forall x \in \Rbb^n.
	\]
	Moreover, the above implies that $\lim_{x: \norm{x} \to \infty}
	f(x) = \infty$, so in particular the sublevel sets of $f$,
	$S_{\alpha}(f) := \set{x \mmid f(x) \leq \alpha}$ are bounded;
	since if they were not, we would be able to find a sequence $\set{x_k}$
	with $\lim_{k \to \infty} \norm{x_k} = +\infty$ with $f(x_k) \leq \alpha$,
	a clear contradiction. Now, it is clear that $\inf_x f(x) = \inf_{x \in
	S_{f(0)}(f)}$, which is the infimum of a convex (hence continuous) function
	on a bounded set. Since $f$ is closed, it follows that its sublevel sets
	are also closed, hence compact. An appeal to the Bolzano-Weierstrass
	Theorem implies that $f$ attains its minimum.
\end{Answer}

\begin{Exercise}
    Consider the problem
    \[
        \inf_{x \in \Rbb^n} \max_{i \in I} f_i(x), \;
        f_i : \Rbb^n \to \Rbb,
    \]
    where $I$ is finite and each $f_i$ is convex and $C^2$-smooth. Suppose
    furthermore that the function $\sum_i f_i$ is strongly convex. By
    considering the problem in the framework of composite optimization, design
    a majorization - minimization algorithm, and prove that it converges under
    reasonable conditions. Try out your algorithm for functions of the form
    \[
        f_i(x) = g_i^\top x + x^\top H_i x, \quad \text{with }
        \sum_i g_i = 0, \; H_i \in \Sbb_{++}^n.
    \]
\end{Exercise}
\begin{Answer}
    Since each $f_i$ is $C^2$-smooth, that means it is twice continuously
    differentiable and $\norm{\grad f_i (y) - \grad f_i (x)} \leq L_i \norm{x -
    y}$. We define the auxiliary functions
    \[
        g\left(
            \pmx{y_1 \\ \vdots \\ y_{\abs{I}}}
        \right) = \max_{i=1, \dots, \abs{I}} y_i, \quad
        H(x) = \pmx{
            f_1(x) \\ \vdots \\ f_{\abs{I}}(x)}
    \]
    Then we can write our problem as a composite optimization problem, since
    $\inf_{x \in \Rbb^n} \max_{i \in I} f_i(x) = \inf_{x \in \Rbb^n} g(H(x))$.
    As discussed in class, a natural candidate for this type of Problem is
    prox-descent, in which we repeat the following iteration:
    \[
        x_{k+1} \in \arg \min_{x \in \Rbb^n} \max_{i \in I} \bar{f}_i(x, x_k) +
        \frac{\lambda}{2} \norm{x - x_k}^2, \; \lambda \geq \max_i
        \frac{L_i}{2},
    \]
    We denote $\bar{f}_i(x, x_k) = f_i(x_k) + \ip{g_k, x - x_k}$, for $g_k \in
    \partial f_i(x_k)$. Since $f_i \in C^2$, we know that $\partial f_i(x_k)
    = \set{\grad f_i(x_k)}$, which makes things simpler for us. Notice that
    \[
        \max_i f_i(x_k) + \ip{g_k, x - x_k} + \frac{\lambda}{2}
        \norm{x - x_k}^2 \geq \max_i f_i(x_k) + \ip{g_k, x - x_k}
        + \frac{L_i}{2} \norm{x - x_k}^2 \geq \max_i f_i(x),
    \]
    by virtue of $f_i$ being $L_i$-smooth, and thus amenable to quadratic upper
    bounds. Therefore our algorithm indeed falls within the scope of
    majorization-minimization algorithms.

    To prove that the above iteration converges to a minimizer, it will suffice
    to show that it converges to a critical point, since the function at hand
    is the maximum of a collection of convex functions, hence itself convex. We
    will verify that $\max_{i \in I} f_i(x)$ and its linearization satisfies
    the assumptions we employed when dealing with the moving balls method.

    \paragraph{1. Existence of a critical point.}
    We first prove that $F(x) := \max_{i \in I} f_i(x)$ has a critical point.
    We claim that $F(x)$ has a minimizer, and since the only critical points of
    convex functions are minimizers, we are then done. Notice that
    \[
        F(x) \geq f_i(x), \; \forall i \Rightarrow
        F(x) \geq \frac{1}{\abs{I}} \sum_i f_i(x)
    \]
    However, we know that $\sum_i f_i$ is strongly convex from the problem
    statement, which is equivalent to $\sum_i f_i - \mu \norm{\cdot}^2$ being
    convex for some scalar $\mu > 0$. Then by Problem 2 we deduce that $\sum_i
    f_i $ has a (unique) minimizer, so it is bounded from below. Consequently,
    $F$ is also bounded from below, and therefore attains a minimizer, since it
    is continuous (piecewise maxima of continuous functions are continuous) and
    its sublevel sets are compact (trivial to see they are bounded, closed by
    an argument about intersections of closed sets, since $f_i$ are $C^2$,
    therefore closed).

    \paragraph{2. The local model is strongly convex.}
    We argue that $h(x, x_k) := \max_{i \in I} f_i(x_k) + \ip{\grad f_i(x_k),
    x - x_k} + \frac{\lambda}{2} \norm{x - x_k}^2$ is strongly convex as a
    function of $x$. This is trivial to show, since $h(x, x_k) -
    \frac{\lambda}{2} \norm{x}^2 = \max_{i \in I} f_i(x_k) +
    \ip{\grad f_i (x_k), x - x_k}$, which is convex as a piecewise maximum of
    affine functions. Appealing to the proof we presented in Problem 2 gives us
    strong convexity of $x \mapsto h(x, x_k)$.

    \paragraph{3. The local model grows quadratically.}
    This is a simple consequence of strong convexity. Since $x_{k+1}
    \in \arg \min_{x \in \Rbb^n} h(x, x_k)$, it must also minimize $h(x, x_k) -
    \frac{\lambda}{2} \norm{x - x_{k+1}}^2$, as the latter is convex. This
    implies that
    \[
        h(x, x_k) - \frac{\lambda}{2} \norm{x - x_{k+1}}^2
        \geq h(x_{k+1}, x_k) - \frac{\lambda}{2} \norm{x_{k+1} - x_{k+1}}^2
        \Rightarrow h(x, x_k) - h(x_{k+1}, x_k) \geq \frac{\lambda}{2}
        \norm{x - x_{k+1}}^2.
    \]
    \paragraph{4. Bounded subgradient.}
    It suffices to show that if $(0, v) \in \partial h(x, y)$ and $x, y$ vary
    in a bounded subset $\cX$, then $\norm{v} \leq K \norm{x - y}$. This
    means that (in an abuse of notation, where we drop the first coordinate):
    \begin{align*}
        v & \in \partial_y \left(\max_{i \in I} \set{  f_i(x, y) + \ip{\grad
        f_i(y),x - y} } + \frac{\lambda}{2} \norm{x - y}^2 \right)
        = \partial \left(
        \max_{i \in I} \set{  f_i(x, x_k) + \ip{\grad
            f_i(x_k),x - x_k} }
        \right) + \lambda (x - y) \\
        &= \lambda (x - y) + \mathrm{conv}\left(
            \partial_y \left( f_i (y) + \ip{\grad f_i(y), x - y} \right)
            \mmid f_i(y) + \ip{\grad f_i(y), x - y} = \max_j
                f_j(y) + \ip{\grad f_j(y), x - y}\right).
    \end{align*}
    The subgradient rule for taking the convex hull was also briefly mentioned
    in one of the ORIE 6328 Homeworks. We now give an explicit formula for the
    subgradients of $f_i(y) + \ip{\grad f_i(y), x - y}$. Since $f_i$ are all
    $C^2$, we write
    \begin{align*}
        \partial \left( f_i(y) + \ip{\grad f_i(y), x - y} \right) &=
        \grad f_i(y) + \grad_y \left( \ip{\grad f_i(y), x - y} \right)
        = \grad f_i(y) - \grad^2 f_i(y)^\top (x - y) - \grad f_i(y) \\
        &= -\grad^2 f_i(y)^\top (x - y)
    \end{align*}
    As $\cX$ is assumed to be bounded, set $K := \max_j \max_{y \in \cX}
    \opnorm{\grad^2 f_j(y)}$. We then have
    \begin{align*}
        \norm{v} &= \norm{\lambda(x - y) - \mathrm{conv}
        \left( \grad^2 f_i(y)^\top (x - y) \mmid f_i(y) = \max_j f_j(y)
        \right)}
        \leq \lambda \norm{x - y} + \max_j \opnorm{\grad^2 f_i(y)}
        \norm{x - y} \\
        &= \left( \lambda + K \right)\norm{x - y}.
    \end{align*}

    \paragraph{5. KL property.}
    Finally, we need to verify that both the original function as well as the
    local upper model satisfy the KL property. This is easy to see for the
    local upper model, which is defined as
    \[
        \max_{i \in I} \set{f_i(x_k) + \ip{\grad f_i(x_k), x - x_k}
        + \frac{\lambda}{2} \norm{ x - x_k}^2}.
    \]
    The graph of the above is the union of at most $\abs{I}$ graphs, one for
    each dominating term. Every individual graph is the graph of an affine plus
    quadratic term, which is a second order polynomial in $\Rbb[x_1, \dots,
    x_n]$, therefore semialgebraic. We conclude that the local upper model
    is semialgebraic and therefore satisfies the KL property around its
    minimizers, as shown e.g. in~\cite{BolDanLew07}.
    On the other hand, the original function is bounded below by a strongly
    convex function, and is itself convex, which means that it admits a
    quadratic minorant. Therefore, we expect it to satisfy the KL property as
    well.

    Having verified the above assumptions, we conclude that our
    majorization-minimization scheme will converge to a critical point of the
    original problem, by arguments outlined during lecture. Since the original
    problem is convex, this critical point will be a minimizer.
\end{Answer}

\begin{Exercise}
    Consider an oracle associated with a compact set $Q \in \Rbb^n$: given any
    vector $g \in \Rbb^n$, the oracle returns an element of the set
    \[
        \argmin_{Q} \ip{g, \cdot}.
    \]
    Suppose I claim that the convex hull of $Q$ does not contain $0$. How could
    you verify my claim, using the bundle technique we described in class? How
    could you use the subgradient method instead? Compare the two methods on
    some random finite sets $Q$.
\end{Exercise}
\begin{Answer}
	In class, we effectively answered the first part of this question when $Q$
	is assumed convex and compact. Here, $Q$ is not necessarily convex, but is
	still compact. Even then, since we are interested in the convex hull
	of $Q$, our oracle is ``sufficient'' in the sense that
	\[
		q \in \argmin_{Q} \ip{g, \cdot}, \; v \in \argmin_{\convhull(Q)} \ip{g,
		\cdot} \Rightarrow \ip{g, q} = \ip{g, v}.
	\]
	If $Q$ is a polytope (or union of polytopes), then the result is immediate
	by the fundamental theorem of linear programming. If not, assume that
	$\exists v \in \convhull(Q), v \notin Q$, such that $\ip{g, v} < \ip{g,
	q}, \; \forall q \in Q$.
	However, since $v \in \convhull(Q)$, there must exist elements
	$v^1, \dots, v^m \in Q, \; \sum_{i=1}^m \lambda_i v^i = v, \; \lambda \geq
	0, \; \bm{1}^\top \lambda = 1$. Then we can write
	\begin{align*}
		\ip{g, v} &= \sum_{i=1}^{m} \lambda_i \ip{g, v^i}
		> \sum_{i=1}^m \lambda_i \ip{g, v} = \ip{g, v},
	\end{align*}
	a contradiction. Then, we can design an algorithm for deciding whether $0
	\in \convhull(Q)$ by adapting the procedure we discussed in lecture using
	the ``modified'' oracle. We denote \textsc{Oracle}($g$) for a call to the
	oracle using the vector $g$.

	\begin{algorithm}[H]
		\caption{Answering $0 \in \convhull(Q)$ using a modified oracle}
		\begin{algorithmic}
			\State $\hat{q} \gets \textsc{Oracle}(0)$; $\hat{Q} =
			\set{\hat{q}}$ \Comment{Initial set}
			\Repeat
				\State $g \gets \mathrm{proj}_{\convhull(\hat{Q})}(0)$
				\If{$\norm{g} = 0$}
					\State \Return ``$0 \in \convhull(Q)$''
				\Else
					\State $\hat{Q} \gets \hat{Q} \cup \set{g}$
				\EndIf
				\State $q \gets \textsc{Oracle}(g)$
				\If{$\ip{g, q} > 0$}
					\State \Return ``$0 \notin \convhull(Q)$''
				\EndIf
				\State $\hat{Q} \gets \hat{Q} \cup \set{q}$
			\Comment{Optional: only keep $\set{g, q}$ at this step}
			\Until Stopping.
		\end{algorithmic}
		\label{alg:modified-oracle}
	\end{algorithm}
	Note that~\cref{alg:modified-oracle} is essentially the same algorithm we
	discussed in lecture. There, the critical step is checking if $\exists v:
	\ip{g, v} \leq 0$, which we can do using our oracle. It holds that
	\[
		\ip{g, q} \leq \ip{g, v}, \; \forall v \in \convhull(Q),
	\]
	so it suffices to check if $\ip{g, q} > 0$; if that is the case, then
	obviously there is no such $v$, so we have found a separating hyperplane
	between $0$ and $\convhull(Q)$. If not, then we have to repeat the
	procedure. Appealing to a Lemma proven in lecture, we know that the
	iterates are stricly decreasing in magnitude, so if $0 \in \convhull(Q)$,
	algorithm will eventually return a positive answer.

	\paragraph{Performing the projection step.} The projection
	step in the loop can be performed by solving a linear program using some
	interior point method. Note that the Lemma we mentioned above shows that
	successive iterates $g$ are decreasing in norm, so then keeping the last
	iterate $g$ and the result $q := \textsc{Oracle}(g)$ will suffice for
	computing the correct projection of $0$ to $\convhull(\hat{Q})$, as this
	projection is effectively calculating the minimum-norm element of
	$\hat{Q}$.


    \paragraph{Using the subgradient method.}
    Denote $\bar{Q} = \convhull(Q)$. Let us consider minimizing the following
    convex program
    \begin{align*}
        \left\{
        \begin{aligned}
        \min & \norm{x}_2 \\
        \mbox{s.t. } & x \in \convhull(Q)
        \end{aligned} \right\}
        = \inf_{x} \set{\norm{x}_2 + \delta_{\bar{Q}}(x)}.
    \end{align*}
    Using Fenchel duality, since $\dom{\norm{\cdot}} = \Rbb^n$, we have that
    \[
        \inf_{x}\set{\norm{x}_2 + \delta_{\bar{Q}}(x)}
        = \sup_{y} \set{- \delta_{\mathbb{B}_2}(y)
        - \sup_{z \in \bar{Q}} \ip{z, -y}  }
        = \sup_{y : \norm{y} \leq 1} \min_{z \in \convhull(Q)} \ip{z, y},
    \]
    where we used the fact that the Fenchel dual of the $\ell_2$ norm is
    the indicator function of the unit ball, and $\left(\delta_{C}\right)^*
    = \sup_{z \in C} \ip{z, \cdot}$, both known from ORIE 6328.
    We can solve the latter problem using the subgradient method. First, we
    reformulate it as $\inf_{y : \norm{y} \leq 1} - \min_{z \in \convhull(Q)}
    \ip{y, z} = \inf_{y : \norm{y} \leq 1} \max_{z \in \convhull(Q)}
    \ip{-y, z}$ which is the same as
    $\min_{y : \norm{y} \leq 1} \max_{z \in \convhull(Q)} \ip{y, z}$ due to
    symmetry of the domain of $y$.

    Furthermore, notice that our oracle is equivalent to an oracle that acts as
    \[
        \textsc{MaxOracle}: g \mapsto \argmax_{q \in \convhull(Q)} \ip{g, q},
    \]
    since $\textsc{Oracle}(-g)$ will return $\argmin_{q \in \convhull{Q}}
    \ip{q, -g}$, which implies
    \[
        \ip{q, -g} \leq \ip{v, -g}, \; \forall v \in \convhull(Q)
        \Rightarrow \ip{q, g} \geq \ip{v, g}, \; \forall v \in \convhull(Q).
    \]
    We showed it suffices to solve the following problem:
    \begin{align}
        \tag{\textsc{MinSupp}}
        \begin{aligned}
            \min & \max_{q \in \convhull(Q)} \ip{g, q} =: f(g) \\
            \mbox{s.t. } & \norm{g} \leq 1.
        \end{aligned}
        \label{prob:minsupp}
    \end{align}
    By Fenchel duality, if $0 \in Q$, the optimal value of the ``primal''
    problem is $0$, so we should expect~\eqref{prob:minsupp} to also have
    zero optimal value. If $0 \notin Q$, the primal problem has strictly
    positive optimal value. The way we transformed the dual problem:
    \[
        \sup \min \dots \Rightarrow - \inf -\min \dots =  -
        \inf \max \dots,
    \]
    reveals that, in that case, the optimal value of~\eqref{prob:minsupp}
    should be strictly negative. This agrees with our previous intuition
    from the bundle method, where we identify a separating hyperplane.
    We can solve Problem~\eqref{prob:minsupp} using the projected subgradient
    method. Obviously, projecting to the unit ball is trivial (a simple scaling
    argument). We can obtain a subgradient using \textsc{MaxOracle}. Notice that
    \begin{align*}
        f(x) &= \max_{q \in \convhull(Q)} \ip{q, x} =
        \max_{q \in \convhull(Q)} \set{\ip{q, x - g} + \ip{q, g}} \geq
        \ip{y, x - g} + \ip{y, g}, \; y \in \argmax_{q \in \convhull(Q)}
        \ip{g, q} \\
        &= f(g) + \ip{y, x - g}, \; \forall x \in \Rbb \Rightarrow
           y \in \partial f(g).
    \end{align*}
    Therefore, we iterate
    \[
        x_{k+1} \gets \mathrm{proj}_{\mathbb{B}_2} \set{
            x_k - \eta \cdot \textsc{MaxOracle}(x_k) }.
    \]
\end{Answer}


\bibliographystyle{plain}
\bibliography{references}
\end{document}
