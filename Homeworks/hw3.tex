\documentclass[10pt]{article}

\input{preamble}
\usepackage{latex-macros}
\usepackage{todonotes}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{algorithm, algpseudocode}
\usepackage[margin=1in]{geometry}
\usetikzlibrary{shapes.geometric}
\usepackage[capitalize]{cleveref}
\usepackage{exercise}
\usepackage{svg}
\setminted{
    linenos=true,
    autogobble,
}
\usepackage{todonotes}

\newcommand{\regdiff}{\hat{\partial}}
\newcommand{\bd}[1]{\mathrm{bd}\left( #1 \right)}
\newcommand{\eps}{\varepsilon}

\begin{document}

\allowdisplaybreaks
\everymath{\displaystyle}
\newenvironment{longlisting}{\captionsetup{type=listing}}{}

\homework{Vasileios Charisopoulos}{Homework 3}{vc333}

\begin{Exercise}
	Consider a closed, proper, convex function $f: \Rbb^n \to \Rbb$. If a
	sequence $\set{(x^k, y^k)}$ converges in the graph of the subdifferential
	$\partial f$, prove that the function values also converge. Are all the
	assumptions about $f$ necessary?
\end{Exercise}
\begin{Answer}
	Convergence in the graph of the subdifferential implies that $\exists
	\set{x^k} \to \xbar$ and $\set{y^k} \to g \in \partial f(\xbar)$ such that
	$y^k \in \partial f(x^k)$. Notice that $\partial f(\xbar) \neq \emptyset$
	since the $\dom f = \Rbb^n$. We now assume that the function values do not
	converge, i.e. $\exists \eps > 0$ such that $\abs{f(x^k) - f(\xbar)} \geq
	\eps$, for all $k \in \Nbb$. Notice that $f$ is assumed to be closed, so
	lower semicontinuous, which implies that $\forall \delta > 0$, there exists
	a ball of radius $\rho > 0$ with $f(x) > f(\xbar) - \delta, \; \forall x \in
	\xbar + \rho \mathbb{B}$. Unfolding the divergence hypothesis means that
	\[
		\abs{f(x^k) - f(\xbar)} \geq \eps \Rightarrow
		\begin{cases}
			f(x^k) \geq f(\xbar) + \eps, & \text{ or } \\
			f(x^k) \leq f(\xbar) - \eps
		\end{cases},
	\]
	but appealing to lower semicontinuity of $f$ with $\delta := \eps$ makes
	the second of the above possibilities not applicable. It remains to show
	that $f(x^k) \geq f(\xbar) + \eps$ is impossible. Notice the following
	chain of inequalities:
	\begin{align*}
		f(x^k) &\geq f(\xbar) + \eps \geq f(x^k) + \ip{y^k, \xbar - x^k} + \eps
			    \geq f(x^k) - \norm{y^k}_2 \norm{\xbar - x^k}_2 + \eps,
	\end{align*}
	where the penultimate inequality is the subgradient inequality with $y^k \in
	\partial f(x^k)$ and the last inequality follows from Cauchy-Schwarz. Now,
	convergence in the subdifferential graph implies that
	\begin{itemize}
		\item $\forall \eps_1, \; \exists K_1$ such that $\norm{x^k - \xbar}
			\leq \eps_1, \; \forall k \geq K_1$, and
		\item $\forall \eps_2, \; \exists K_2$ such that $\norm{y^k - g}
			\leq \eps_2, \; \forall k \geq K_2$, where $g \in \partial
			f(\xbar)$.
	\end{itemize}
	Finally, an appeal to local Lipschitzness of $f$, which follows since $f$ is
	convex, implies that $\exists \eta > 0$ such that $\norm{g} \leq L$ for
	some $L > 0$, which is identified by the local Lipschitz modulus.
	Therefore:
	\[
		\norm{y^k} \leq \norm{y^k - g} + \norm{g} \leq \norm{y^k - g} + L,
	\]
	and we can set $\eps_2 := \kappa, \; \eps_1 := \frac{\eps}{2 (\kappa + L)}$,
	so that for large enough $k$, we have
	\begin{align*}
		\norm{y^k} \norm{x^k - \xbar} &\leq \eps_1 \eps_2 \leq
		\frac{\eps}{2 (\kappa + L)} \cdot (\kappa + L) \leq \frac{\eps}{2} \\
		\Rightarrow f(x^k) \geq f(x^k) - \frac{\eps}{2} + \eps = f(x^k) +
		\frac{\eps}{2},
	\end{align*}
	which is a contradiction since $\eps > 0$. Therefore, $f$ must converge in
	function value. Notice that we have used the fact that $f$ is proper
	implicitly in the sense that our $f(x^k)$ are finite.

	We could follow the same reasoning without explicitly requiring $f$ to be
	closed; since $\dom f = \Rbb^n$, our point $\xbar \in \intr{\dom f}$, otherwise an
	``approaching'' sequence $\set{x_k}$ must, in fact, diverge. This
	immediately implies that $f$ is lower semicontinuous around $\xbar$ by
	virtue of being locally Lipschitz continuous.
\end{Answer}

\begin{Exercise}
	Consider a closed proper function $f: \Rbb^n \to \Rbb$ with the property
	that the function $f - \norm{\cdot}^2$ is convex. Prove that $f$ has a
	unique minimizer.
\end{Exercise}
\begin{Answer}
	We first give an elementary proof of the fact that $f$ is strongly convex.
	Applying the algebraic criterion of convexity to $f - \norm{\cdot}^2$ gives
	us, for $\lambda \in (0, 1), \; x \neq y$:
	\begin{align*}
		f(\lambda x + (1 - \lambda) y) - \norm{\lambda x + (1 - \lambda) y}^2
		&\leq \lambda f(x) + \lambda \norm{x}^2 + (1 - \lambda) f(y) + (1 -
		\lambda) \norm{y}^2 \nonumber \\
		f(\lambda x + (1 - \lambda) y) - \lambda^2 \norm{x}^2 + (1 - \lambda)^2
		\norm{y}^2 - 2 \lambda(1 - \lambda) \ip{x, y} &\leq
			\lambda f(x) + (1 - \lambda) f(y) - \lambda \norm{x}^2 - (1 -
			\lambda) \norm{y}^2,
	\end{align*}
	so we arrive at
	\begin{align}
		f(\lambda x + (1 - \lambda) y) &\leq
		\lambda f(x) + (1 - \lambda) f(y) - (\lambda - \lambda^2) \norm{x}^2
		- \left( (1 - \lambda) - (1 - \lambda)^2 \right) \norm{y}^2
		+ 2 \lambda (1 - \lambda) \ip{x, y}.
		\label{eq:f-ineq}
	\end{align}
	Now, notice that we can simplify $(1 - \lambda) - (1 - \lambda)^2 =
	(1 - \lambda)( 1 - (1 - \lambda)) = \lambda(1 - \lambda)$ and also $\lambda
	- \lambda^2 = \lambda(1 - \lambda)$, so that
	\begin{align*}
		-(\lambda - \lambda^2) \norm{x}^2 - \left( (1 - \lambda) - (1 -
		\lambda)^2 \right) \norm{y}^2 + 2 \lambda(1 - \lambda) \ip{x, y} &=
		-\lambda (1 - \lambda) \left(\norm{x}^2 + \norm{y}^2 - 2 \ip{x,
		y}\right) \\
			&= -\lambda(1 - \lambda) \norm{x - y}^2,
	\end{align*}
	which is strictly negative when $x \neq y$ and $\lambda \in (0, 1)$. This
	implies the following, when combined with~\cref{eq:f-ineq}:
	\[
		f(\lambda x + (1 - \lambda) y) < \lambda f(x) +
		(1 - \lambda) f(y), \; \forall x \neq y, \; \lambda \in (0, 1).
	\]
	Now, we will prove that $f$ has \textit{at most} one minimizer. Suppose
	that $\exists x_1 \neq x_2$ such that $f(x_1) = f(x_2) \leq f(z), \;
	\forall z \in \Rbb^n$ (it is clear that minimizers have the same function
	value due to the fact that $f$ is convex).
	Then, for some $0 < t < 1$, set $z := t x_1 + (1 - t) x_2$, for which we
	know:
	\begin{align*}
		f(t x_1 + (1 - t) x_2) & < t f(x_1) + (1 - t) f(x_2) = t f(x_1) + (1 - t)
		f(x_1) = f(x_1) \Rightarrow f(z) < f(x_1),
	\end{align*}
	a clear contradiction to our assumption. Therefore, $f$ has \textbf{at
	most} one minimizer.

	It remains to prove that $f$ attains a minimizer. Assume, on the contrary,
	that $f$ does not attain a minimizer. Given the convexity of $f -
	\norm{\cdot}^2$, we have that
	\begin{align*}
		f(x) - \norm{x}^2 &\geq \ip{\partial \left(f - \norm{\cdot}^2
		\right)(0), x - 0} = \ip{\partial f(0), x} - 2 \ip{0, x} =
		\ip{\partial f(0), x},
	\end{align*}
	where the existence of $\partial g$ follows from the fact that its domain
	is the whole underlying space and we use the sum rule of subdifferential
	calculus for the sum of a convex ($f$) and a differentiable
	($\norm{\cdot}^2$) function. This means that
	\[
		f(x) \geq \norm{x}^2 + \ip{\partial f(0), x}, \; \forall x \in \Rbb^n.
	\]
	Moreover, the above implies that $\lim_{x: \norm{x} \to \infty}
	f(x) = \infty$, so in particular the sublevel sets of $f$,
	$S_{\alpha}(f) := \set{x \mmid f(x) \leq \alpha}$ are bounded;
	since if they were not, we would be able to find a sequence $\set{x_k}$
	with $\lim_{k \to \infty} \norm{x_k} = +\infty$ with $f(x_k) \leq \alpha$,
	a clear contradiction. Now, it is clear that $\inf_x f(x) = \inf_{x \in
	S_{f(0)}(f)}$, which is the infimum of a convex (hence continuous) function
	on a bounded set. Since $f$ is closed, it follows that its sublevel sets
	are also closed, hence compact. An appeal to the Bolzano-Weierstrass
	Theorem implies that $f$ attains its minimum.
\end{Answer}


\bibliographystyle{plain}
\bibliography{references}
\end{document}
