\documentclass[10pt]{article}

\input{preamble}
\usepackage{latex-macros}
\usepackage{todonotes}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{algorithm, algpseudocode}
\usepackage[margin=1in]{geometry}
\usetikzlibrary{shapes.geometric}
\usepackage[capitalize]{cleveref}
\usepackage{exercise}
\usepackage{svg}
\setminted{
    linenos=true,
    autogobble,
}
\usepackage{todonotes}

\newcommand{\regdiff}{\hat{\partial}}
\newcommand{\bd}[1]{\mathrm{bd}\left( #1 \right)}

\begin{document}

\allowdisplaybreaks
\everymath{\displaystyle}
\newenvironment{longlisting}{\captionsetup{type=listing}}{}

\homework{Vasileios Charisopoulos}{Homework 2}{vc333}

\section*{Problem 1}
\paragraph{1.}
For the first part, we want to prove three properties:
\begin{itemize}
\item $d_{C}$ is \textbf{proper} since $\norm{x - P_C(x)} \geq 0$ by the
nonnegativity of the norm.
\item $d_{C}$ is \textbf{convex}: take an arbitrary convex combination $
\lambda x + (1 - \lambda) y$ and notice that $P_C(x), P_C(y) \in C \Rightarrow
\lambda P_C(x) + (1 - \lambda) P_C(y) \in C$, by convexity of $C$. Then:
\begin{align*}
    d_C(\lambda x + (1 - \lambda)y) &= \inf_{z \in C}
    \norm{\lambda x + (1 - \lambda) y - z} \leq
    \norm{\lambda x + (1 - \lambda) y - \lambda P_C(x) - (1 - \lambda) P_C(y)}
    \\
    &= \norm{\lambda(x  - P_C(x)) + (1 - \lambda)(y - P_C(y))} \leq
       \lambda \norm{x - P_C(x)} + (1 - \lambda) \norm{y - P_C(y)} \\
    &=   \lambda d_C(x) + (1 - \lambda) d_C(y).
\end{align*}
\item $d_C$ is \textbf{closed}: take a sequence
$\set{z_k}_{k=1}^{\infty} \to \bar{z}$. Then, we have
\[
    \lim_{k \to \infty} \norm{d_C(z_k) - d_C(\bar{z})} =
    \lim_{k \to \infty} \norm{x - P_C(z_k) - \bar{z} - P_C(\bar{z})} \leq
    2 \lim_{k \to \infty} \norm{z_k - \bar{z}} = 0,
\]
by the 1-Lipschitz property of the projection map (proved in e.g. \textsc{Orie
6328}). Hence $d_C$ is closed.
\end{itemize}

\paragraph{2.}
Notice that since $d_C$ is closed, convex and proper, it holds that $d_C =
d_C^{**}$. The following implication holds:
\[
	d_{C}^* = \delta_{\mathbb{B}} + \delta_{C}^* \Leftrightarrow
	d_{C} = \left( \delta_{\mathbb{B}} + \delta_{C}^* \right)^*.
\]
It suffices to prove the latter, which combined with an argument about Fenchel
duality will let us pass to the conjugate. We write
\begin{align}
	d_{C}(x) &= \inf_{t \in C} \norm{x - t} =
		\inf_{t \in C} \sup_{z \in \mathbb{B}} \ip{z, x - t} \nonumber \\
		&= \sup_{z \in \mathbb{B}} \inf_{t \in C} \ip{z, x - t}
		 = \sup_{z \in \mathbb{B}} \set{\ip{z, x} - \sup_{t \in C} \ip{x, t}}
		\label{eq:fench-dual} \\
		&= \sup_{z \in \mathbb{B}} \set{\ip{z, x} - \delta_{C}^*(x)} =
		   \sup_{z \in \Rbb^n} \set{\ip{z, x} - \delta_{\mathbb{B}}(x) -
		   \delta_C^*(x)} \nonumber
\end{align}
Now, it suffices to justify the interchange of $\sup / \inf$
in~\cref{eq:fench-dual}. This follows from Fenchel duality: indeed, write the
first form as
\begin{align*}
	\inf_{t \in C} \sup_{z \in \mathbb{B}} \ip{z, x - t} &=
		\inf_{t} \set{\delta_{C}(t) + \norm{x - t}}.
\end{align*}
Writing $f(t) = \delta_{C}(t), g(t) = \norm{x - t}$, and noticing that the
domains of $f, g$ intersect in a stable fashion, Fenchel duality tells us that
\begin{align*}
	\inf_{t} f(t) + g(t) &= \sup_{y \in \Rbb^n} -f^*(y) - g^*(-y) =
		\sup_{y \in \Rbb^n} -\delta_{C}^*(y) - g^*(-y). \\
	g^*(-y) &= \sup_{z \in \Rbb^n} \ip{z, -y} - \norm{x - z} =
		\sup_{z \in \Rbb^n} \ip{x - z, y} - \ip{x, y} - \norm{x - z} \\
		&= -\ip{x, y} + \sup_{z \in \Rbb^n} \ip{x - z, y} - \norm{x - z}
		 = -\ip{x, y} + \sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} \\
		&= -\ip{x, y} + \delta_{\mathbb{B}}(y) \\
	\inf_t f(t) + g(t) &= \sup_{y \in \Rbb^n} \ip{x, y} -
	\delta_{\mathbb{B}}(y) - \delta_{C}^*(y) = \sup_{y \in \mathbb{B}}
		\inf_{p \in C} \ip{x - p, y},
\end{align*}
which completes the claim. In the above, we made use of the auxiliary result
that is explained in the sequel. First, consider
\begin{align*}
    \sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} &\geq
    \ip{\lambda y, y} - \lambda \norm{y} = \lambda
    \left( \norm{y}^2 - \norm{y}\right)
\end{align*}
Therefore, if $y \notin \mathbb{B}$, the above can be driven to $+\infty$ by
choosing $\lambda \to \infty$. On the other hand, if $y \in \mathbb{B}$, we have
$\sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} \leq \sup_{p \in \Rbb^n} \norm{p}
\norm{y} - \norm{p} = \sup_{p \in \Rbb^n} \left( \norm{y} - 1 \right)\norm{p}$,
which is nonpositive since $\norm{y} \leq 1$. In that case, the supremum is
attained for $p = 0$, hence we conclude
\[
    \sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} = \delta_{\mathbb{B}}(y).
\]

\paragraph{3.}
By the Fenchel-Young inequality, we know that
\[
    d_C^*(x) + d_C^{**}(y) \geq \ip{x, y} \Leftrightarrow
    d_C^*(x) + d_C(y) \geq \ip{x, y},
\]
Using the Proposition proved in class, we know that $y \in \partial d_C^*(x)
\Leftrightarrow x \in \partial d_C^{**}(y) = \partial d_C(y)$, since $d_C$ is
closed, convex and proper and therefore equal to its biconjugate. Therefore,
we know that
\begin{align*}
    \partial d_{C}(x) &= \set{z \mmid d_{C}(x) + d^*_{C}(z) = \ip{z, x}}
    \overset{\text{(Part 2)}}{=}
    \set{z \mmid d_{C}(x) = \ip{z, x} - \delta_{\mathbb{B}}(z) - \sup_{t \in C}
    \ip{t, z}}
\end{align*}
Now, consider the following cases:
\begin{itemize}
\item $x \in C$: in that case, we know that $d_{C}(x) = 0$, and therefore we
must have
\begin{align*}
    0 &= \ip{z, x} - \delta_{\mathbb{B}}(z) - \sup_{t \in C} \ip{t, z}
    \Rightarrow z \in \mathbb{B},
\end{align*}
otherwise the RHS is $-\infty$. Additionally, as soon as $z \in \mathbb{B}$, we
obtain
\[
    \ip{z, x} = \sup_{t \in C} \ip{t, z} \Rightarrow
    \ip{z, x} \geq \ip{z, t}, \; \forall t \in C \Rightarrow
    \ip{z, t - x} \leq 0, \; \forall t \in C.
\]
However, this is precisely the definition of a normal cone for a convex set,
which concludes that $z \in \mathrm{N}_{C}(x)$.
\item $x \notin C$: in that case, the LHS is nonzero and we write
\begin{align*}
    d_{C}(x) &= \ip{z, x} - \delta_{\mathbb{B}}(z) - \sup_{t \in C} \ip{t, z}
    \Leftrightarrow d_{C}(x) = \ip{z, x} - \sup_{t \in C} \ip{t, z}, \; z \in
    \mathbb{B} \\
        &= \inf_{t \in C} \ip{x - t, z}
\end{align*}
For $z = \frac{x - t}{\norm{x - t}}$, we obtain $d_{C}(x) = \inf_{t \in C}
\norm{x - t} = d_{C}(x)$ when $t = P_C(x)$, leading to $z = \frac{1}{d_{C}(x)}
(x - P_C(x))$.
\end{itemize}
\paragraph{4.}
In HW1, we showed that a closed, proper, convex function $f$ is (Gateaux)
differentiable at a point $x \in \intr{\dom f}$ iff its
subdifferential is a singleton. Moreover, in that case, its gradient is equal
to its Gateaux derivative. Now, $f(x) = \frac{1}{2} \norm{x - P_C(x)}^2$ is
obviously proper since $f(x) \geq 0$ and closed as the product of two closed
functions. Finally, it is convex as the composition of $x^2$ and $d_C(x)$.
This gives us
\[
    d^2_C(\lambda x + (1 - \lambda) y) =
    \left( d_C(\lambda x + (1 - \lambda) y) \right)^2 \leq
    \left( \lambda d_C(x) + (1 - \lambda) d_C(y) \right)^2 \leq
    \lambda d_C^2(x) + (1 - \lambda) d_C(y)^2,
\]
where in the above steps we made use of the fact that $x^2$ is nondecreasing on
$[0, +\infty)$ and $d_C$ is convex for the first inequality, and the fact that
$x^2$ is convex for the second inequality.

Assuming that we've proved that $\grad \left(\frac{1}{2} d_C^2\right)(x) =
x - P_C(x)$, verifying its $1$-Lipschitzness is simple:
\begin{align*}
    \norm{\grad \frac{d_C^2}{2}(x) - \grad \frac{d_C^2}{2}(y)} &=
    \norm{x - P_C(x) - (y - P_C(y))} = \norm{x - y + (P_C(y) - P_C(x))} \\
    &= \norm{x - y} - 2\ip{x - y, P_C(x) - P_C(y)} + \ip{P_C(x) - P_C(y)} \\
    &= \norm{x - y} + 2 \underbrace{\ip{y - P_C(y), P_C(x) - P_C(y)}}_{\leq 0}
       + 2 \underbrace{\ip{x - P_C(x), P_C(y) - P_C(x)}}_{\leq 0} \\
    &\leq \norm{x - y},
\end{align*}
where in the above we've made use of the following fundamental
property\footnote{Proved in HW1 of ORIE 6328, Spring 2018}:
\[
    \ip{z - P_C(z), s - P_C(z)} \leq 0, \; \forall s \in C.
\]
In order to show that $\grad \frac{d_C^2}{2}(x) = x - P_C(x)$, we prove that
\[
    d_C^2(x + th) = d_C^2(x) + 2 \ip{\grad d_C(x), th} + o(th), \;
    h \in \Sbb^{n_1}.
\]
We consider cases for $x$, but we first prove that $\partial d_C^2(x)
\subseteq N_C(x)$ when $x \in C$. Assume that $\exists g \notin N_C(x)$ with $g
\in \partial d_C^2(x)$, which implies that for some $t \in C$ it holds that
$\ip{g, t - x} > 0$. However, this gives us
\begin{align*}
    d_C^2(x + h) &\geq d_C^2(x) + \ip{g, h} \Rightarrow
    d_C^2(x + t - x) \geq d_C^2(x) + \ip{g, t - x} \Rightarrow \\
    0 &\geq 0 + \ip{g, t - x} \Rightarrow \ip{g, t - x} \leq 0,
\end{align*}
which is a contradiction.
\begin{itemize}
\item $x \notin C$: in that case, we know from part (3) and the subgradient
inequality that
\begin{align*}
    d_C(x + h) &\geq d_C(x) + \ip{\frac{1}{d_C(x)} x - P_C(x), h} \Rightarrow
    d_C(x + h) d_C(x) \geq d^2_C(x) + \ip{x - P_C(x), h}.
\end{align*}
Moreover, since the subgradient is a singleton there, it follows that we
can rewrite the above as an inequality modulo an $o(h)$ error term.
It remains to write the LHS as something more ``friendly'' to $d_C^2(x + h)$.
From the AM-GM inequality ($\sqrt{xy} \leq (x + y)/2$), we obtain
\begin{align*}
    \frac{d^2_C(x + h)}{2} + \frac{d_C^2(x)}{2} &\geq d_C^2(x) +
    \ip{x - P_C(x), h} + o(h) \Rightarrow
    \frac{d_C^2(x + h)}{2} \geq \frac{d_C^2(x)}{2} + \ip{x - P_C(x), h}
    + o(h), \; \forall h.
\end{align*}
The above simply implies that $ x - P_C(x) \in \partial \left(
\frac{d_C^2(x)}{2} \right), x \notin C$. However, we know that there are cases
for which the AM-GM inequality is attained, and since we demand the above
$\forall h$, this means that $\exists \bar{h}$ such that $d_C(x+\bar{h}) d_C(x)
= \frac{d_C^2(x)}{2} + \frac{d_C^2(x + \bar{h})}{2}$. Therefore, it holds that
\[
    \frac{d_C^2(x + h)}{2} = \frac{d_C^2(x)}{2} + \ip{x - P_C(x), h}
    + o(h), \; \forall h \in \Rbb^n, \; x \notin C.
\]
\item $x \in C$: First, notice that if $x \in \intr{C}$, $d_C$ is zero in
an $\epsilon$-ball around $x$, which implies $d_C^2$ is differentiable
with zero gradient at $x$. It thus suffices to consider $x \in C \setD
\intr{C}$. Since $x = P_C(x)$, we write
\begin{align*}
    d_C^2(x + th) - \cancelto{0}{d_C^2(x)} &= \left( x + th - P_C(x + th)
        - x + P_C(x) \right)^\top \left(x + th - P_C(x + th) - x +
        P_C(x)\right)\\
      &= \left(th - P_C(x + th) + P_C(x)\right)^\top \left(x + th - P_C(x + th)
        +x - P_C(x) \right) \\
      &= t h^\top (x - P_C(x)) + th^\top (x - P_C(x + th)) + t^2 h^\top h \\
      &+ \left[ P_C(x + th) - P_C(x) \right]^\top \left(x + th - P_C(x + th)
         - x + P_C(x) \right)
\end{align*}
Break up the last summand into two terms:
\[
    (P_C(x + th) - P_C(x))^\top \left(P_C(x) - P_C(x + th)\right), \;
    (P_C(x + th) - P_C(x))^\top th
\]
We have
\begin{align*}
    \lim_{t \dto 0} \frac{(P_C(x + th) - P_C(x))^\top th}{t} &=
        \lim_{t \dto 0} \left( P_C(x + th) - P_C(x) \right)^\top h = 0, \\
    0 \geq \lim_{t \dto 0} \frac{(P_C(x + th) - P_C(x))^\top \left(P_C(x) -
    P_C(x +
    th)\right)}{t} &\geq \lim_{t \dto 0} -\frac{t^2 \norm{h}^2}{t} = 0
\end{align*}
Therefore, taking the directional derivative we have
\begin{align*}
    \lim_{t \dto 0} \frac{d_C^2(x + th) - d_C^2(x)}{t} &=
    \lim_{t \dto 0} \frac{t h^\top(x - P_C(x)) + t h^\top (x - P_C(x + th))
        + t^2 \norm{h}^2}{t} \\ &=
    h^\top (x - P_C(x)) + \lim_{t \dto 0}  h^\top (x - P_C(x + th)) +
        t \norm{h}^2 \\
    &= 2 h^\top (x - P_C(x)),
\end{align*}
where we used the fact that $t \mapsto P_C(x + th)$ is a continuous function
by the continuity of projection. Since the directional derivative is a
singleton, it must hold that $\grad d_C^2(x) = 2(x - P_C(x)) \Rightarrow
\grad \left( \frac{d_C^2(x)}{2}\right) = x - P_C(x)$, which completes the claim.
\end{itemize}
\section*{Problem 2}
\paragraph{1.}
We can write for the gradient of $f$ at iterate $x_k$:
\begin{align*}
    \grad f(x_k) &= \frac{1}{m} \sum_{i=1}^{m} \grad \left(
    \frac{d^2_{C_i}}{2} \right)(x_k) = \frac{1}{m} \sum_{i=1}^m x_k -
    P_{C_i}(x_k) = x_k - \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k) \\
    \Leftrightarrow
    x_k - \grad f(x_k) &= x_k - x_k + \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k)
        = \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k),
\end{align*}
which is essentially the update rule if we consider applying gradient descent
with step size $\eta_k = 1$ to the function $f$.

\paragraph{2.}
We know that $\grad f(x_k)$ is $1$-Lipschitz from Question (1.4).
Additionally, we have from part (2.1) that $x_{k+1} = x_k - \grad f(x_k)$.
Combine the above just like in the last question of HW1 to obtain
\begin{align*}
    \norm{\grad f(x_{k+1})} - \norm{\grad f(x_k)} &\leq
    \norm{\grad f(x_k) - \grad f(x_{k+1})} \leq \norm{x_k - x_{k+1}} \\
    \norm{\grad f(x_{k+1})} &\leq \norm{\grad f(x_k)} + \norm{x_k - x_{k+1}}
        = 2 \norm{x_k - x_{k+1}},
\end{align*}
which is one of the postulates needed to characterize slope descent sequences,
as a differentiable convex function satisfies $\abs{\grad f}(x_+) =
\norm{\grad f(x_+)}$. For the latter postulate, we notice that $f$ is a
$1$-smooth function by virtue of gradient Lipschitzness, therefore we can
use the quadratic upper bound to conclude
\begin{align*}
    f(x_k) - f(x_{k+1}) &\geq \ip{\grad f(x_{k}), x_k - x_{k+1}} - \frac{1}{2}
    \norm{x_k - x_{k+1}}^2 \\
    &\geq \ip{x_k - x_{k+1}, x_k - x_{k+1}} - \frac{1}{2} \norm{x_k - x_{k+1}}^2
    = \frac{1}{2} \norm{x_k - x_{k+1}}^2,
\end{align*}
which shows that both of the postulates for $\set{x_k}_{k \in \Nbb}$ to be a
slope descent sequence are satisfied.

\paragraph{3.}
Denote $d \triangleq 2 \max_i d_{C_i}(x)$ and set $u_i \triangleq P_{C_i}(x)$.
We know that $\norm{u_i} \leq \norm{x}, \; \forall i$, since
\[
    0 \in C_i \Rightarrow
    \norm{x - u_i} \leq \norm{x - \lambda u_i}, \; \forall \lambda \in [0, 1],
\]
which leads to $\norm{u_i} \leq \norm{x}$ after squaring and a limiting
argument. Take $v_1 := u_1 - u_2$, which satisfies
\[
    \norm{u_1 - u_2} = \norm{u_1 - x + x - u_2} \leq d.
\]
Then, $\frac{\delta}{d} v_1 \in \setI_{i=1}^m C_i$ since $\delta \mathbb{B}
\subset \setI_i C_i$. Moreover, the vector $p_1 = \frac{\delta}{\delta + d} u_1
= \frac{\delta}{\delta + d} u_1 + \frac{d}{\delta + d} 0 \in C_1$, by convexity
of $C_1$. Moreover, $p_1 \in C_2$, appealing again to the convexity of $C_2$:
\[
    p_1 = \underbrace{\frac{d}{\delta + d} \frac{\delta}{d} (u_1 - u_2)}_{\in
    C_2} + \underbrace{\frac{\delta}{\delta + d} u_2}_{\in C_2} \in C_2
\]
Therefore
\begin{align*}
    \mathrm{dist}(x, C_1 \cap C_2) &\leq \mathrm{dist}(x, p_1)
    = \norm{x - p_1} \leq \norm{x - u_1 + u_1 - p_1} \leq \frac{d}{2}
    + \norm{\frac{d}{\delta + d} u_1} \\
    &\leq \frac{d}{2} \left(1 + \frac{2 \norm{u}}{\delta + d}\right)
     \leq \left(1 + \frac{2 \norm{x}}{\delta}\right) \max_i d_{C_i}(x).
\end{align*}
We repeat the above construction for $C_3, \dots, C_m$. Specifically, since
$p_1 \in C_1 \cap C_2$, we seek a point $p_2 \in C_1 \cap C_2 \cap C_3$
constructed in terms of $p_1, u_3$. We then argue that
\[
    \mathrm{dist}(x, \setI_{i=1}^3 C_i) \leq \norm{x - p_2} \leq
    \norm{x - p_1 + p_1 - p_2} \leq \norm{x - p_1} + \norm{p_2 - p_1}.
\]
It suffices for $p_2$ to be such that $\norm{p_1 - p_2} \leq 2 \norm{x}
\delta^{-1} \left( \norm{x - p_1} \right)$ to make use of our result above.

\paragraph{4.}
This part is simple. From the form of the updates, we obtain that
\begin{align*}
    x_{k+1} &= \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k) =
        \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k) - P_{C_i}(0) \quad (P_{C_i}(0) =
        0) \\
    \norm{x_{k+1}} &\leq \frac{1}{m} \sum_{i=1}^m \norm{P_{C_i}(x_k) -
    P_{C_i}(0)} \leq \frac{1}{m} \sum_{i=1}^m \norm{x_k - 0} = \norm{x_k},
\end{align*}
where we used the $1$-Lipschitzness of projections to convex sets and the fact
that $0 \in \setI_i C_i \Rightarrow P_{C_i}(0) = 0, \; \forall i$.

\paragraph{5.}
Pick a point $x \in \Rbb^n$ such that $\norm{x} \leq \norm{x_0}$. Combined with
the bound from part (2.3), we obtain that
\[
    d_{\setI_i C_i}(x) \leq \left(1 + \frac{2 \norm{x}}{\delta}\right)^{m-1}
    \max_i d_{C_i}(x) \leq \left(1 + \frac{2 \norm{x_0}}{\delta}\right)^{m-1}
    \max_i d_{C_i}(x),
\]
since we assume $m \geq 2$. It remains to show that $\max_i d_{C_i}(x) \leq
\sqrt{2 m f(x)} = \sqrt{\sum_{i=1}^m d_{C_i}^2(x)}$. However, this follows
since the RHS satisfies
\begin{align*}
    \sqrt{\sum_{i=1}^m d_{C_i}^2(x)} &\geq \sqrt{d_{C_j}^2(x)}, \; \forall j
    \Rightarrow \sqrt{\sum_{i=1}^m d_{C_i}^2(x)} \geq \max_j \sqrt{d_{C_j}^2(x)}
    = \max_i d_{C_i}(x).
\end{align*}

\paragraph{6.}
Like we did in class, we deduce the KL property from the error bound above.
Take $y = \mathrm{proj}_{\setI_i C_i} (x)$. Obviously, $f(y) = 0$ so $y \in
\argmin f$. Then, using the subgradient inequality, we obtain
\begin{align*}
    f(y) - f(x) &\geq \ip{\grad f(x), y - x} \Rightarrow
    f(x) \leq \norm{\grad f(x)} \norm{x - y} = \norm{\grad f(x)}
    \mathrm{dist}(x, \argmin f),
\end{align*}
which follows since $y$ is the projection of $x$ to the set of minimizers of
$f$, which is precisely the set of points in $\setI_i C_i$. By the bound from
part (2.5), and the fact that $\abs{\grad f}(u) = \norm{\grad f(u)}$ for $C^1$
functions, the above becomes
\begin{align}
    \begin{aligned}
    f(x) &\leq \norm{\grad f(x)} \mathrm{dist}(x, \argmin f) \leq
    \abs{\grad f}(x) \left(1 + \frac{2 \norm{x_0}}{\delta}\right)^{m-1}
    \sqrt{2m f(x)} \Rightarrow \\
    1 &\leq \abs{\grad f}(x) \left(1 + \frac{2 \norm{x_0}}{\delta}\right)^{m-1}
        \sqrt{\frac{2m}{f(x)}},
    \end{aligned} \label{eq:KL-prop}
\end{align}
which is \textbf{almost} the KL property with desingularizing function
$\varphi(s) = 2 \left(1 + \frac{2 \norm{x_0}}{\delta}\right)^{m-1} \sqrt{2m}
\sqrt{s}$. It is easy to verify that $\varphi(s)$ is concave on $(0, +\infty)$
and increasing as a scalar multiple of the $\sqrt{~}$ function. Moreover, if
we extend it so that
\begin{align*}
    \varphi(s) &= \begin{cases}
        0, & s = 0 \\
        2\left(1 + \frac{2 \norm{x_0}}{\delta}\right)^{m-1} \sqrt{\frac{2m}{s}},
        & s > 0
    \end{cases},
\end{align*}
we obtain $\varphi(0) = 0$, concave and increasing on $\Rbb_{++}$, as the KL
property demands. Since $\varphi(s) = ks^{1/2}$, we expect the algorithm to
converge linearly to a minimizer $x_* \in \setI_i C_i$.

Starting from the error bound obtained in part (2.5), we assume that we have
a bounded sequence $\set{x_k}$ starting from $x_0$. Indeed, we proved in (2.4)
that the sequence of iterates satisfies $\norm{x_{k+1}} \leq \norm{x_k}, \;
\forall k$, which verifies our assumption. By arguments presented in lecture,
specifically during the part about KL and complexity, we know that $\set{x_k}
\overset{\text{norm}}{\to} x^*$, where $x^*$ is a minimizer of $f$, i.e.
$x_* \in \setI_{i=1}^m C_i$. Denote
\(
    q := \left(1 + \frac{2 \norm{x_0}}{\delta}\right)^{m-1} \sqrt{2m}
\) and write
\begin{align*}
    \norm{x_k - x_*}^2 &\leq \sum_{t=k}^{\infty} \norm{x_t - x_{t + 1}}^2
    \leq 2 \sum_{t=k}^{\infty} f(x_t) - f(x_{t+1}) = 2 (f(x_k) -
    \cancelto{0}{f(x_*)}) \\
    &\leq 2 \set{ \abs{\grad f}(x_k) q }^2 \leq 4q^2 \norm{x_k -
    x_{k-1}}^2,
\end{align*}
by virtue of being a slope descent sequence. However, if we write $g_k =
\sum_{t=k}^{\infty} \norm{x_t - x_{t+1}}^2$, the above recurrence reads
\[
    g_k \leq 4 q^2 \left(g_{k-1} - g_{k}\right) \Rightarrow
    g_k \leq \frac{4q^2}{1 + 4q^2} g_{k-1} \Rightarrow
    g_k \leq \left(\frac{4q^2}{1 + 4q^2}\right)^k g_0
\]
where $g_0 = \sum_{t=0}^{\infty} \norm{x_t - x_{t+1}}^2$ is \textbf{finite} by
the arguments presented in lecture. Therefore, it holds that
\[
    \norm{x_k - x_*} \leq \sqrt{g_0} \left( \frac{4q^2}{1 + 4 q^2}
    \right)^{k/2},
\]
which is linear convergence with rate $\sqrt{\frac{4q^2}{1 + 4q^2}}$.
\section*{Problem 3}
\paragraph{1.}
For this part, notice that if we assume $f: \Hbb \to \overline{\Rbb}$ with
$f(\bar{x})$ finite, if $\bar{x}$ is not a critical point, then there must
exist an upper slice $U(\epsilon) = \set{u \in \Rbb^d \mmid f(\bar{x}) < f(u) <
f(\bar{x}) + \epsilon}$ and a $\varepsilon$ ball around $\bar{x}$ such that no
point in $\mathbb{B}(\bar{x}, \varepsilon) \cap U(\epsilon)$ is a critical
point. Otherwise, $\forall \epsilon, \varepsilon$, we would be able to find a
sequence $u_k \to \bar{u}$, $f(u_k) \to f(\bar{u})$
with $w_k \in \hat{\partial} f(u_k), w_k \to \partial f(\bar{u}), \norm{w_k}
\to 0$, with $\norm{f(\bar{u}) - f(\bar{x})} < \epsilon, \; \norm{\bar{u} -
\bar{x}} < \varepsilon$. However, this would imply that
\[
    \norm{f(\bar{x}) - f(u_k)} \leq \norm{f(\bar{x}) - f(\bar{u})}
    + \norm{f(\bar{u}) - f(u_k)} < \epsilon' + \epsilon'', \; \forall
    \epsilon', \epsilon'' < \epsilon
\]
and also that
\[
    \norm{\bar{x} - u_k} \leq \norm{\bar{x} - \bar{u}} + \norm{\bar{u}- u_k}
    \leq \varepsilon' + \varepsilon'' < \varepsilon, \; \forall
    \varepsilon', \varepsilon'' < \varepsilon.
\]
This implies that $\exists w_k \in \hat{\partial} f(u_k), \; f(u_k) \to
f(\bar{x}), u_k \to \bar{x}$ with $\norm{w_k} \to 0$, which means that $0 \in
\partial f(\bar{x})$, a clear contradiction.
Name those ``safe limits'' $\epsilon_*, \varepsilon_*$. Then, we know that
\[
    \mathrm{dist}_{\partial f(u)}(0) \geq \kappa, \; \forall u
    \in U(\epsilon_*) \cap \mathbb{B}(\bar{x}, \varepsilon_*),
\]
for some $\kappa > 0$, implying that $\phi(s) = \kappa^{-1} s$ satisfies the
KL property at $\bar{x}$ with $\phi(0) = 0, \phi' = \kappa^{-1} > 0$, defined
on a slice $U(\epsilon_*)$ in the neighbourhood $\mathbb{B}(\bar{x},
\varepsilon_*)$, since
\[
    \phi'(f(u) - f(\bar{x})) \mathrm{dist}_{\partial f(u)}(0) >
    \kappa^{-1} \kappa \geq 1.
\]

\paragraph{2.}
Given our discussion about semialgebraic functions, we must restrict ourselves
to functions with non-semialgebraic graphs. That automatically excludes all
polynomial or piecewise polynomial functions.
Moreover, we must restrict ourselves to the family of functions that satisfy
$f(0) = 0, \;  \grad f(0) = 0$, otherwise $0$ is a non-critical point and part
(a) applies. Fortunately the condition $f(x) \geq x^4$ takes care of this.

Consider $f(x) = \begin{cases}
    0, & x = 0 \\
    x^4(2 + \sin(1/x)), & x \neq 0
\end{cases}$. We have $0$ is a critical point for $f$ since $f(x) \geq f(0) =
0, \; \forall x$. Additionally, we know that $f(x)$ is differentiable for $x
\neq 0$ and differentiable at $0$ after taking the limit-theoretic definition
of the derivative. Finally $\abs{\sin(1/x)} \leq 1$ so it follows that $f(x)
\geq x^4$, as required. Assume, for the sake of contradiction, that there
exists a desingularizer $\phi: [0, \rho)
\to \Rbb_+$ with $\phi' > 0$ for some $\rho > 0$ such that
\[
    \abs{\grad (\varphi \circ f)}(x) \geq 1, \; \forall x: 0 < f(x)
    < \rho.
\]
Here, $f$ is $C^1$ and $\varphi$ is also $C^1$. By the chain rule, we have that
\[
    \abs{\grad \varphi \circ f}(x) = \varphi'(f(x) )\abs{\grad f}(x) \geq 1
\]
where the latter follows since $\abs{\grad f}(x) = \norm{\grad f(x)}$ by our
results from HW1.

\section*{Problem 4}
Consider $\cZ = \set{\bar{z} \in \Rbb^n \mmid \lim_{k \in K \subset \Nbb} z_{k}
= \bar{z}}$, where $\set{z_k}$ is an arbitrary bounded sequence that satisfies
$\norm{z_k - z_{k+1}} \to 0$. We have to prove the following properties:
\begin{itemize}
\item nonemptiness: this follows trivially, since $\set{z_k}$ is a bounded
sequence and by the Bolzano-Weierstrass Theorem admits at least one convergent
subsequence.
\item compactness: $\cZ$ has to be a bounded set, since otherwise we would be
able to find a sequence $\set{\bar{z}_k}, \norm{\bar{z}_k} \to \infty$. This
would imply the existence of an unbounded subsequence $\set{z_{j_k}} \to
\bar{z}_k$, which contradicts the construction of $\cZ$.
Additionally, $\cZ$ is defined as the closure of all convergent subsequences of
$\set{z_k}$ with the aforementioned properties, hence itself closed.
\item connectedness: assume, for the sake of contradiction, that $\cZ$ was not
a connected set. This means that there exists a pair $U, V$ such
that $\cZ = U \cup V$ with $U \cap V = \emptyset$. This implies that
$\exists \epsilon: \inf_{u \in U, v \in V} \norm{u - v} > \epsilon$. However,
this means that there exist two subsequences of $\set{z_k}$, such that
\begin{align*}
    u_{k} \to u, \; v_{k} \to v, \; \norm{u - v} > \epsilon, \text{ so} \\
    \epsilon < \norm{u_k - u} + \norm{v_k - v} + \norm{u_k - v_k} \Rightarrow
    \epsilon < \lim_{k \to \infty} \norm{u_k - v_k} = 0
\end{align*}
since $v_k = z_{k + i_k}, u_k = z_{k + j_k}$ for some $i_k, j_k$ and in the
limit we make use of the property that $\lim_{k \to \infty} \norm{z_{k+1} -
z_k} = 0$. This is a clear contradiction.

\end{itemize}
Finally, we prove that $d_{\cZ}(z_k) \to 0$. We write
\begin{align*}
    \lim_{k \to \infty} d_{\cZ}(z_k) &=
        \lim_{k \to \infty} \inf_{z \in \cZ} \norm{z - z_k}
\end{align*}
Assume for the sake of contradiction that the above quantity was strictly
positive, i.e. $\lim_{k \to \infty} \inf_{z \in \cZ} \norm{z - z_k} > \epsilon$.
However, by construction of $\cZ$, $z$ has to be a limit point of a convergent
subsequence $\set{z_k}_{k \in K \subseteq \Nbb}$. This implies that (using the
continuity of $x \mapsto \| \lim_{k \in K} z_k - x \|$):
\[
    \lim_{i \to \infty} \norm{\lim_{k \in K} z_k - z_i} > \epsilon \Rightarrow
    \norm{\lim_{k \in K} z_k - \lim_{i \to \infty} z_i} > \epsilon,
\]
a clear contradiction to the fact that $\lim_{k \to \infty} \norm{z_k - z_{k+1}}
= 0$.

\section*{Problem 5}
\paragraph{Limiting Subdifferential.}
Define $f(u, v) = \abs{u} - \abs{v}$, which does not have a regular subgradient
at $(0, 0)$ since $-\abs{v}$ admits no regular subgradient at $0$. Therefore, we
must deal with sequences of approaching regular subgradients along paths where
$\regdiff f(u, v)$ exists and approaches a limit. Consider the following cases:
\begin{itemize}
\item $u, v \neq 0$: in this case, $f$ is differentiable with $\regdiff f (u,
v) = \grad f (u, v) = \sign(u) - \sign(v)$.
\item $u = 0, v \neq 0$: in this case, $\partial \abs{u}(0)= [-1, 1]$ by known
results about convex subdifferentials, so the limiting subdifferential will
depend on the approach along $(0, v)$. The sum rule gives us
\[
    \regdiff f(u, v) \subset \regdiff \abs{u} + \regdiff (-\abs{v}) =
    \begin{pmatrix} [-1, 1] \\ 0 \end{pmatrix} + \begin{pmatrix}
        0 \\ \regdiff (-\abs{v}) \end{pmatrix} =
    \begin{pmatrix}
        [-1, 1] \\ \sign(v)
    \end{pmatrix}
\]
Notice that the above inclusion is actually an equality, since we can plug in
any element from the RHS in the definition of $\regdiff f$ and verify the
reverse inclusion. We have that $(g_1, g_2)^\top \in \regdiff f(0, v)$ if
\begin{align*}
    f(0 + \lambda_1, v + \lambda_2) &=
        \abs{\lambda_1} - \abs{v + \lambda_2} \geq
        0 - \abs{v} + g_1 \lambda_1 + g_2 \lambda_2 + o\left(
        \bm{\lambda}\right).
\end{align*}
Notice that, for any $g_1 \in [-1, 1]$, we have that $\abs{\lambda_1} \geq g_1
\lambda_1$. Moreover, setting $g_2 = \sign(v) = \grad (-\abs{v})$ satisfies by
the local linearization property of the gradient:
\[
    -\abs{v + \lambda_2} \geq -\abs{v} + \sign(v) \lambda_2 + o(\lambda_2),
\]
therefore we conclude that $\regdiff f(0, v) = \pmx{[-1, 1] \\ \sign(v)}$.
\item $v = 0, u \neq 0$: in that case, we show that $-\abs{v}$ has no regular
subdifferential. If that existed, then we would have $g$ such that
\[
    -\abs{d} \geq -\abs{0} + g d + o(d) \Rightarrow \abs{d} + g d + o(d) \leq 0,
    \; \forall d \in \Rbb^n
\]
If $g = 0$, we get $\abs{d} + o(d) \leq 0$, which is impossible. If $g \neq 0$,
for $d = \lambda \sign(g), \lambda > 0$, we obtain the contradiction
\[
    \lambda (1 + \abs{g}) + o(\lambda) \leq 0 \Rightarrow
    \lim_{\lambda \to 0^+} \frac{\lambda (1 + \abs{g})}{\norm{\lambda}}
    + \cancelto{0}{\frac{o(\lambda)}{\norm{\lambda}}} \leq 0 \Rightarrow
    1 + \abs{g} \leq 0.
\]
\end{itemize}
Combining the above approaches, we conclude that
\[
    \partial f(u, v) = [-1, 1] \times \set{-1, 1}
\]
since for $u = 0, v \neq 0$ we can find approaching sequences $(0, v_k), (0,
w_{\ell}) \to (0, 0)$ with $v_k > 0$ and $w_{\ell} < 0$ which lead to sequences
of regular subdifferentials $\partial f(0, v_k) = [-1, 1] \times \set{1}, \;
\partial f(0, w_{\ell}) = [-1, 1] \times \set{-1}$.


\paragraph{Limiting Slope.}
For the limiting slope, we know that
\[
    \overline{\abs{\grad f}}(0, 0) = \liminf_{\substack{u \to (0, 0) \\ f(u)
    \to f(0, 0)}} \abs{\grad f}(u) = \liminf_{\substack{u \to (0, 0) \\ f(u)
    \to 0}} \abs{\grad f}(u).
\]
For any point $u \neq (0, 0)$, we have
\begin{align*}
\abs{\grad f}(u) &=
\limsup_{z \to u} \frac{f(u) - f(z)}{\norm{u - z}} = \lim_{\epsilon \to 0}
\sup_{z : \norm{z - u} \leq \epsilon}
\frac{\abs{u_1} - \abs{u_2} - \abs{z_1} + \abs{z_2}}{\norm{u - z}},
\end{align*}
and, like before, we approach on a case-by-case basis, depending on which
quadrant $u$ is located in. If both coordinates are nonzero, then $f$ is
differentiable at $u$, which means that
\[
    \abs{\grad f}(u) = \norm{\grad f}(u) = \norm{\mathrm{sign}(u)} = \sqrt{2},
    \; \text{when } u_1, u_2 \neq 0
\]
If $u_1 = 0, u_2 \neq 0$ we still know that $f$ is
subdifferentiable in the Frech{\'e}t sense at $u$, which gives us
\begin{align*}
    \limsup_{z \to u} \frac{f(u) - f(z)}{\norm{u - z}} &\leq
    \limsup_{z \to u} \frac{\ip{\regdiff f(u), u - z} + o(u - z)}{\norm{u - z}}
    = \limsup_{z \to u} \frac{\ip{\regdiff f(u), u - z}}{\norm{u - z}} \\
    \limsup_{z \to u} \frac{\pmx{[-1, 1] \\ \pm 1}^\top (u - z)}{\norm{u - z}}
    &= \limsup_{z \to u} \frac{[-1, 1] (u_1 - z_1) \pm (u_2 - z_2)}{\norm{u -
    z}} \leq \frac{\norm{u - z} \cdot \norm{g}}{\norm{u - z}} = \norm{g}, \;
    \forall g \in \regdiff f(u).
\end{align*}
The shortest subgradient in the above is $g = 0 \times \set{\pm 1}$ with
$\norm{g} = 1$, so $\abs{\grad f}(u) \leq 1, $ when $u_1 = 0, u_2 \neq 0$. At
the same time, we know that $\abs{\grad f}(u) \geq -f'(u; h) = \lim_{t \to 0}
\frac{f(u) - f(u + th)}{t}$ for every direction $h \in \mathbb{B}_2$. Choosing
$h = (0, 1)$ gives us $-f'(u; h) = 1$, which shows that $\abs{\grad f}(u) = 1$
when $u_1 = 0, u_2 \neq 0$.

If $u_1 \neq 0, u_2 = 0$, $f$ is not Frech{\'e}t subdifferentiable at $u$, so
we cannot resort to the subgradient inequality. However, we still know that the
slope dominates the negative of the directional derivative, so
\begin{align*}
    \abs{\grad f}((u_1, 0)) &\geq \lim_{t \dto 0}
    \frac{f(u) - f(u + th)}{t} = \lim_{t \dto 0} \frac{\abs{u_1} - \abs{u_1 +
    th_1} + \abs{th_2}}{t} \geq 1,
\end{align*}
where the last part follows if we choose $h = (\pm 1, 0)$, which implies that
$\abs{u_1 + th_1}$ has the same sign as $\abs{u_1}$ for small enough $t$,
leading to $\abs{u_1} - \abs{u_1 + th_1} = \pm t h_1 = \pm t$, depending on the
sign of $u_1$.

All of the approaches to $u$ above result in slopes lower bounded by $1$.
Therefore, we conclude that
\[
    \liminf_{\substack{u \to (0, 0) \\ f(u) \to 0}} \abs{\grad f}(u) \geq 1
    = \norm{0 \times \set{\pm 1}}
\]
which is attained if we choose the approaching sequence to be $u_n := (0,
u_2^n), \; u_2^n \to 0$ with slopes $\abs{\grad f}(u_n) = 1$ and function
values approaching $f(0, 0) = 0$. This agrees with
the fact stated in class, i.e. that the limiting slope is equal to the norm of
the shortest limiting subgradient.

\section*{Problem 6}
Consider the unconstrained low-rank matrix factorization problem:
\[
    \inf\set{\frac{1}{2}\norm{A - X}_F^2, \; \rank(X) \leq r}
\]
The Eckhart-Young theorem suggests that the optimal low-rank approximation is
given by
\[
    X_* = U \mathrm{diag}(\sigma_1, \dots, \sigma_r, 0, \dots 0) V^\top,
\]
where $A = U \mathrm{diag}(\sigma_1, \dots, \sigma_n) V^\top$ is the singular
value
decomposition of $A$, where the singular values are ordered as $\sigma_1 \geq
\sigma_2 \geq \dots \geq \sigma_n$.
On the other hand, the alternating proximal gradient applied to
\[
    \Psi(X, Y) = \frac{1}{2} \norm{A - XY}_F^2
\]
yields the following updates:
\begin{algorithm}
    \caption{Alternating gradients for low-rank approximation}
    \begin{algorithmic}
        \Repeat
            \State $X_{k+1} \gets \prox{\lambda \delta_{\Rbb^{n \times r}}}{
                X_k - \frac{1}{\lambda} \grad_X \Psi(X_k, Y_k)}$
            \State $Y_{k+1} \gets \prox{\nu \delta_{\Rbb^{r \times n}}}{
                Y_k - \frac{1}{\nu} \grad_Y \Psi(X_{k+1}, Y_k)}$
        \Until{convergence}
    \end{algorithmic}
\end{algorithm}
However, $\prox{\lambda \delta_{\Hbb}}{x} = x$ when $\Hbb$ is the underlying
space, therefore the above reduces to alternating gradient descent steps.
We write
\[
    \grad_X \Psi(X, Y) = (XY - A) Y^\top, \; \grad_Y \Psi(X, Y) = X^\top (XY -
    A).
\]
Notice that the gradient is separately Lipschitz since we have
\begin{align*}
    \norm{\grad_{X} \Psi(X_1, Y) - \grad_X \Psi(X_2, Y)}_F &=
        \norm{X_1 Y Y^\top - X_2 Y Y^\top}_F \leq \norm{YY^\top}_F
        \norm{X_1 - X_2}_F = \norm{Y}_F^2 \norm{X_1 - X_2}_F
\end{align*}
and similarly for $\grad_Y$. Setting $\lambda = \gamma \max\set{\kappa_1,
\norm{Y_k}_F^2}, \nu = \gamma \max\set{\kappa_2,
\norm{X_{k+1}}_F^2}$, with $\kappa_1 > 0, \kappa_2 > 0, \gamma > 1$, we
obtain the convergence plots shown in~\cref{fig:rankA-15,fig:rankA-30}, where
all matrices are initialized as Gaussian ensembles. The full code can be found
in Listing~\ref{p6code}.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.49 \textwidth}
    \centering
    \begin{tikzpicture}% table
    \begin{axis}[xlabel=$k$,ylabel={$\Psi(X_k, Y_k)$}, ymode=log,
                 legend pos=north east, width=\linewidth]
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_50_rank_15.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_100_rank_15.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_150_rank_15.csv};
    \legend{\scalebox{0.75}{$n = 50$}, \scalebox{0.75}{$n = 100$},
            \scalebox{0.75}{$n = 150$}};
    \end{axis}
    \end{tikzpicture}
    \caption{Error plots for $\rank(A) = 15$}
    \label{fig:rankA-15}
    \end{minipage}~
    \begin{minipage}{0.49 \textwidth}
    \centering
    \begin{tikzpicture}% table
    \begin{axis}[xlabel=$k$,ylabel={$\Psi(X_k, Y_k)$}, ymode=log,
                 legend pos=south west, width=\linewidth]
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_50_rank_30.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_100_rank_30.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_150_rank_30.csv};
    \legend{\scalebox{0.75}{$n = 50$}, \scalebox{0.75}{$n = 100$},
            \scalebox{0.75}{$n = 150$}};
    \end{axis}
    \end{tikzpicture}
    \caption{Error plots for $\rank(A) = 30$}
    \label{fig:rankA-30}
    \end{minipage}
\end{figure}

We point out that the distance of the final iterate in instances where the rank
of the matrix $A$ is small compared to the ambient dimension $n$ is at least as
good as the SVD solution, for a moderate value of iterations. When we increase
the rank $r$ compared to $n$, the proximal gradient method needs more
iterations to converge to a solution. However, given enough iterations, it
always seems to produce a global minimizer since the final error is close to
$0$.

\begin{longlisting} \label{p6code}
	\inputminted{python}{hw2_p6.py}
	\caption{Script for Problem 6}
	% \label{p6code}
\end{longlisting}

To deduce the rate of convergence, we assume that $\set{Z_k}_{k=1}^{\infty}$ is
a bounded sequence of points, so that the sufficient decrease and approximate
criticality properties follow. Let $\tau_1 = \sup_{k} \norm{X_k}_F, \;
\tau_2 = \sup_{k} \norm{Y_k}_F$ be the corresponding upper bounds.
\begin{itemize}
\item The sufficient decrease property is satisfied by the Lemma shown in class.
Specifically, the quantity $\max\set{\norm{Y_k}_F^2, \kappa_1}$ is a
Lipschitz constant for $\grad_X H(\cdot, Y_k)$, and similarly $
\max\set{\norm{X_{k+1}}_F^2, \kappa_2}$ is a Lipschitz constant for $\grad_Y
H(\cdot, Y_k)$, with
\[
    \alpha := \min\set{\inf_k \left(1 - \frac{1}{\gamma}\right)
        \max\set{\norm{Y_k}_F^2, \kappa_1},
        \inf_k \left(1 - \frac{1}{\gamma}\right)
        \max\set{\norm{X_{k+1}}_F^2, \kappa_2}} =
   \left(1 - \frac{1}{\gamma}\right) \min \set{\kappa_1, \kappa_2}
\]
satisfying
\[
    \Psi(Z_{k+1}) - \Psi(Z_k) \geq \frac{\alpha}{2}
    \norm{Z_{k+1} - Z_k}^2_F.
\]
\item We can also deduce approximate criticality since the gradient is
Lipschitz. We have
\begin{align*}
    \norm{\grad \Psi(Z_k)} &= \norm{\pmx{
        (X_k Y_k - A) Y_k^\top \\
        X_{k+1}^\top (X_{k+1} Y_k - A)
    }}
\end{align*}
\end{itemize}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}
We prove that $\Psi$ satisfies the KL property on a neighbourhood of a
critical point $\bar{Z}$ with desingularizing function $\phi(s) = ks^{1/2}$.
A critical point $\bar{Z} = (\Xbar, \Ybar)$ will satisfy

\begin{align*}
    \norm{\grad \Psi(\Zbar)} &= 0 \Leftrightarrow
    \norm{\pmx{(\Xbar \Ybar - A) \Ybar^\top \\ \Xbar^\top (\Xbar \Ybar - A)}} =
    0 \Rightarrow
    \begin{cases}
        (\Xbar \Ybar - A) \Ybar^\top = 0 \\
        \Xbar^\top (\Xbar \Ybar - A) = 0
    \end{cases} \\
    \Rightarrow &
        \Xbar \Ybar - A \in \mathrm{range}(\Xbar), (\Xbar \Ybar - A)^\top \in
        \mathrm{Null}(\Ybar)
\end{align*}
The composition $\phi \circ (\Psi(Z) - \Psi(\Zbar))$ is equal to
\[
    \frac{k}{\sqrt{2}} \sqrt{\norm{A - XY}_F^2 - \norm{A - \Xbar \Ybar}_F^2}
\]
Indeed, in a neighbourhood $U$ around $\Zbar$ with $
\Psi(\Zbar) < \Psi(Z) < \Psi(\Zbar) + \eta$ for positive $\eta$ such that
$Z \in U \cap \set{Z \mmid \Psi(\Zbar) < \Psi(Z) < \Psi(\Zbar) + \eta}$,
we employ the fact that $\mathrm{dist}(0, \partial \Psi(Z)) =
\mathrm{dist}(0, \grad \Psi(Z)) = \norm{\grad \Psi(Z)}$ to deduce
\begin{align*}
    \phi'(\Psi(Z) - \Psi(\Zbar)) \norm{\grad \Psi(Z)} &=
    \frac{k}{2} \frac{\sqrt{2}}{\sqrt{\norm{A - XY}_F^2 - \norm{A - \Xbar
    \Ybar}_F^2}}
    \norm{\pmx{(X Y - A)Y^\top \\ X^\top (XY - A)}} \\
    &= \frac{k}{\sqrt{2}} \frac{1}{\sqrt{\norm{A - XY}_F^2 - \norm{A - \Xbar
    \Ybar}_F^2}} \sqrt{\norm{(XY - A)Y^\top}^2 + \norm{X^\top(XY - A)}^2}
\end{align*}
If we expand the denominator, we obtain from the criticality conditions
\begin{align*}
    \norm{A - \Xbar \Ybar}_F^2 &=
    \mathrm{Tr}\left[ (A - \Xbar \Ybar)^\top A - \underbrace{(A -
    \Xbar\Ybar)^\top X}_{= 0} Y \right]
    = \mathrm{Tr}\left( A^\top A -\Ybar^\top \underbrace{\Xbar^\top A}_{=
    \Xbar^\top \Xbar Y} \right) \\
    &= \norm{A}_F^2 - \mathrm{Tr}(\Ybar^\top
    \Xbar^\top \Xbar \Ybar) \Rightarrow \\
    \norm{A - XY}_F^2 - \norm{A - \Xbar \Ybar}_F^2 &=
    \norm{A - XY}_F^2 - \norm{A}^2 + \mathrm{Tr}(\Ybar^\top \Xbar^\top
    \Xbar \Ybar) \\
    &= \mathrm{Tr}(Y^\top X^\top X Y) - \mathrm{Tr}(A^\top XY) -
       \mathrm{Tr}(Y^\top X^\top A) + \mathrm{Tr}(\Ybar^\top \Xbar^\top
       \Xbar \Ybar) \\
    &= \norm{XY}_F^2 + \norm{\Ybar \Xbar}_F^2 - 2 \mathrm{Tr}(A^\top XY)
\end{align*}

\end{document}
