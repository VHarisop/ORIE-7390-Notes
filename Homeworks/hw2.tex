\documentclass[10pt]{article}

\input{preamble}
\usepackage{latex-macros}
\usepackage{todonotes}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{algorithm, algpseudocode}
\usepackage[margin=1in]{geometry}
\usetikzlibrary{shapes.geometric}
\usepackage[capitalize]{cleveref}
\usepackage{exercise}
\usepackage{svg}
\setminted{
    linenos=true,
    autogobble,
}
\usepackage{todonotes}

\newcommand{\regdiff}{\hat{\partial}}
\newcommand{\bd}[1]{\mathrm{bd}\left( #1 \right)}

\begin{document}

\allowdisplaybreaks
\everymath{\displaystyle}
\newenvironment{longlisting}{\captionsetup{type=listing}}{}

\homework{Vasileios Charisopoulos}{Homework 2}{vc333}

\section*{Problem 1}
\paragraph{1.}
For the first part, we want to prove three properties:
\begin{itemize}
\item $d_{C}$ is \textbf{proper} since $\norm{x - P_C(x)} \geq 0$ by the
nonnegativity of the norm.
\item $d_{C}$ is \textbf{convex}: take an arbitrary convex combination $
\lambda x + (1 - \lambda) y$ and notice that $P_C(x), P_C(y) \in C \Rightarrow
\lambda P_C(x) + (1 - \lambda) P_C(y) \in C$, by convexity of $C$. Then:
\begin{align*}
    d_C(\lambda x + (1 - \lambda)y) &= \inf_{z \in C}
    \norm{\lambda x + (1 - \lambda) y - z} \leq
    \norm{\lambda x + (1 - \lambda) y - \lambda P_C(x) - (1 - \lambda) P_C(y)}
    \\
    &= \norm{\lambda(x  - P_C(x)) + (1 - \lambda)(y - P_C(y))} \leq
       \lambda \norm{x - P_C(x)} + (1 - \lambda) \norm{y - P_C(y)} \\
    &=   \lambda d_C(x) + (1 - \lambda) d_C(y).
\end{align*}
\item $d_C$ is \textbf{closed}: take a sequence
$\set{z_k}_{k=1}^{\infty} \to \bar{z}$. Then, we have
\[
    \lim_{k \to \infty} \norm{d_C(z_k) - d_C(\bar{z})} \leq
    \lim_{k \to \infty} \norm{z_k - \bar{z}} = 0,
\]
by the $1$-Lipschitz property of the distance map (proved in e.g. \textsc{Orie
6328}). Hence $d_C$ is closed.
\end{itemize}

\paragraph{2.}
Notice that since $d_C$ is closed, convex and proper, it holds that $d_C =
d_C^{**}$. The following implication holds:
\[
	d_{C}^* = \delta_{\mathbb{B}} + \delta_{C}^* \Leftrightarrow
	d_{C} = \left( \delta_{\mathbb{B}} + \delta_{C}^* \right)^*.
\]
It suffices to prove the latter, which combined with an argument about Fenchel
duality will let us pass to the conjugate. We write
\begin{align}
	d_{C}(x) &= \inf_{t \in C} \norm{x - t} =
		\inf_{t \in C} \sup_{z \in \mathbb{B}} \ip{z, x - t} \nonumber \\
		&= \sup_{z \in \mathbb{B}} \inf_{t \in C} \ip{z, x - t}
		 = \sup_{z \in \mathbb{B}} \set{\ip{z, x} - \sup_{t \in C} \ip{x, t}}
		\label{eq:fench-dual} \\
		&= \sup_{z \in \mathbb{B}} \set{\ip{z, x} - \delta_{C}^*(x)} =
		   \sup_{z \in \Rbb^n} \set{\ip{z, x} - \delta_{\mathbb{B}}(x) -
		   \delta_C^*(x)} \nonumber
\end{align}
Now, it suffices to justify the interchange of $\sup / \inf$
in~\cref{eq:fench-dual}. This follows from Fenchel duality: indeed, write the
first form as
\begin{align*}
	\inf_{t \in C} \sup_{z \in \mathbb{B}} \ip{z, x - t} &=
		\inf_{t} \set{\delta_{C}(t) + \norm{x - t}}.
\end{align*}
Writing $f(t) = \delta_{C}(t), g(t) = \norm{x - t}$, and noticing that the
domains of $f, g$ intersect in a stable fashion, Fenchel duality tells us that
\begin{align*}
	\inf_{t} f(t) + g(t) &= \sup_{y \in \Rbb^n} -f^*(y) - g^*(-y) =
		\sup_{y \in \Rbb^n} -\delta_{C}^*(y) - g^*(-y). \\
	g^*(-y) &= \sup_{z \in \Rbb^n} \ip{z, -y} - \norm{x - z} =
		\sup_{z \in \Rbb^n} \ip{x - z, y} - \ip{x, y} - \norm{x - z} \\
		&= -\ip{x, y} + \sup_{z \in \Rbb^n} \ip{x - z, y} - \norm{x - z}
		 = -\ip{x, y} + \sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} \\
		&= -\ip{x, y} + \delta_{\mathbb{B}}(y) \\
	\inf_t f(t) + g(t) &= \sup_{y \in \Rbb^n} \ip{x, y} -
	\delta_{\mathbb{B}}(y) - \delta_{C}^*(y) = \sup_{y \in \mathbb{B}}
		\inf_{p \in C} \ip{x - p, y},
\end{align*}
which completes the claim. In the above, we made use of the auxiliary result
that is explained in the sequel. First, consider
\begin{align*}
    \sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} &\geq
    \ip{\lambda y, y} - \lambda \norm{y} = \lambda
    \left( \norm{y}^2 - \norm{y}\right)
\end{align*}
Therefore, if $y \notin \mathbb{B}$, the above can be driven to $+\infty$ by
choosing $\lambda \to \infty$. On the other hand, if $y \in \mathbb{B}$, we have
$\sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} \leq \sup_{p \in \Rbb^n} \norm{p}
\norm{y} - \norm{p} = \sup_{p \in \Rbb^n} \left( \norm{y} - 1 \right)\norm{p}$,
which is nonpositive since $\norm{y} \leq 1$. In that case, the supremum is
attained for $p = 0$, hence we conclude
\[
    \sup_{p \in \Rbb^n} \ip{p, y} - \norm{p} = \delta_{\mathbb{B}}(y).
\]

\paragraph{3.}
By the Fenchel-Young inequality, we know that
\[
    d_C^*(x) + d_C^{**}(y) \geq \ip{x, y} \Leftrightarrow
    d_C^*(x) + d_C(y) \geq \ip{x, y},
\]
Using the Proposition proved in class, we know that $y \in \partial d_C^*(x)
\Leftrightarrow x \in \partial d_C^{**}(y) = \partial d_C(y)$, since $d_C$ is
closed, convex and proper and therefore equal to its biconjugate. Therefore,
we know that
\begin{align*}
    \partial d_{C}(x) &= \set{z \mmid d_{C}(x) + d^*_{C}(z) = \ip{z, x}}
    \overset{\text{(Part 2)}}{=}
    \set{z \mmid d_{C}(x) = \ip{z, x} - \delta_{\mathbb{B}}(z) - \sup_{t \in C}
    \ip{t, z}}
\end{align*}
Now, consider the following cases:
\begin{itemize}
\item $x \in C$: in that case, we know that $d_{C}(x) = 0$, and therefore we
must have
\begin{align*}
    0 &= \ip{z, x} - \delta_{\mathbb{B}}(z) - \sup_{t \in C} \ip{t, z}
    \Rightarrow z \in \mathbb{B},
\end{align*}
otherwise the RHS is $-\infty$. Additionally, as soon as $z \in \mathbb{B}$, we
obtain
\[
    \ip{z, x} = \sup_{t \in C} \ip{t, z} \Rightarrow
    \ip{z, x} \geq \ip{z, t}, \; \forall t \in C \Rightarrow
    \ip{z, t - x} \leq 0, \; \forall t \in C.
\]
However, this is precisely the definition of a normal cone for a convex set,
which concludes that $z \in \mathrm{N}_{C}(x)$.
\item $x \notin C$: in that case, the LHS is nonzero and we write
\begin{align*}
    d_{C}(x) &= \ip{z, x} - \delta_{\mathbb{B}}(z) - \sup_{t \in C} \ip{t, z}
    \Leftrightarrow d_{C}(x) = \ip{z, x} - \sup_{t \in C} \ip{t, z}, \; z \in
    \mathbb{B} \\
        &= \inf_{t \in C} \ip{x - t, z}
\end{align*}
For $z = \frac{x - t}{\norm{x - t}}$, we obtain $d_{C}(x) = \inf_{t \in C}
\norm{x - t} = d_{C}(x)$ when $t = P_C(x)$, leading to $z = \frac{1}{d_{C}(x)}
(x - P_C(x))$.
\end{itemize}
\paragraph{4.}
The Lipschitzness of the gradient follows since
\begin{align*}
    \norm{\grad \frac{d_C^2}{2}(x) - \grad \frac{d_C^2}{2}(y)} &=
    \norm{x - P_C(x) - (y - P_C(y))} = \norm{x - y + (P_C(y) - P_C(x))} \\
    &\leq \norm{x - y} + \norm{P_C(y) - P_C(x)} \leq 2 \norm{x - y}.
\end{align*}

\section*{Problem 2}
\paragraph{1.}
We can write for the gradient of $f$ at iterate $x_k$:
\begin{align*}
    \grad f(x_k) &= \frac{1}{m} \sum_{i=1}^{m} \grad \left(
    \frac{d^2_{C_i}}{2} \right)(x_k) = \frac{1}{m} \sum_{i=1}^m x_k -
    P_{C_i}(x_k) = x_k - \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k) \\
    \Leftrightarrow
    x_k - \grad f(x_k) &= x_k - x_k + \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k)
        = \frac{1}{m} \sum_{i=1}^m P_{C_i}(x_k),
\end{align*}
which is essentially the update rule if we consider applying gradient descent
with step size $\eta_k = 1$ to the function $f$.

\paragraph{2.}
We know that $\grad f(x_k)$ is $1$-Lipschitz from Question (1.4).
Additionally, we have from part (2.1) that $x_{k+1} = x_k - \grad f(x_k)$.
Combine the above just like in the last question of HW1 to obtain
\begin{align*}
    \norm{\grad f(x_{k+1})} - \norm{\grad f(x_k)} &\leq
    \norm{\grad f(x_k) - \grad f(x_{k+1})} \leq \norm{x_k - y_k} \\
    \norm{\grad f(x_{k+1})} &\leq \norm{\grad f(x_k)} + \norm{x_k - y_k}
        = 2 \norm{x_k - y_k},
\end{align*}
which is one of the postulates needed to characterize slope descent sequences,
as a differentiable convex function satisfies $\abs{\grad f}(x_+) =
\norm{\grad f(x_+)}$. For the latter postulate, we notice that $f$ is a
$1$-smooth function by virtue of gradient Lipschitzness, therefore we can
use the quadratic upper bound to conclude
\begin{align*}
    f(x_k) - f(x_{k+1}) &\geq \ip{\grad f(x_{k}), x_k - x_{k+1}} - \frac{1}{2}
    \norm{x_k - x_{k+1}}^2 \\
    &\geq \ip{x_k - x_{k+1}, x_k - x_{k+1}} - \frac{1}{2} \norm{x_k - x_{k+1}}^2
    = \frac{1}{2} \norm{x_k - x_{k+1}}^2,
\end{align*}
which shows that both of the postulates for $\set{x_k}_{k \in \Nbb}$ to be a
slope descent sequence are satisfied.

\paragraph{3.}
Denote $d \triangleq \max_i d_{C_i}(x)$ and set $u_i \triangleq P_{C_i}(x)$.
We know that $\norm{u_i} \leq \norm{x}, \; \forall i$, since
\[
    0 \in C_i \Rightarrow
    \norm{x - u_i} \leq \norm{x - \lambda u_i}, \; \forall \lambda \in [0, 1],
\]
which leads to $\norm{u_i} \leq \norm{x}$ after squaring and a limiting
argument.

\section*{Problem 3}
\paragraph{1.}
For this part, notice that if we assume $f: \Hbb \to \overline{\Rbb}$ with
$f(\bar{x})$ finite, if $\bar{x}$ is not a critical point, then there must
exist an upper slice $U(\epsilon) = \set{u \in \Rbb^d \mmid f(\bar{x}) < f(u) <
f(\bar{x}) + \epsilon}$ and a $\varepsilon$ ball around $\bar{x}$ such that no
point in $\mathbb{B}(\bar{x}, \varepsilon) \cap U(\epsilon)$ is a critical
point. Otherwise, $\forall \epsilon, \varepsilon$, we would be able to find a
sequence $u_k \to \bar{u}$, $f(u_k) \to f(\bar{u})$
with $w_k \in \hat{\partial} f(u_k), w_k \to \partial f(\bar{u}), \norm{w_k}
\to 0$, with $\norm{f(\bar{u}) - f(\bar{x})} < \epsilon, \; \norm{\bar{u} -
\bar{x}} < \varepsilon$. However, this would imply that
\[
    \norm{f(\bar{x}) - f(u_k)} \leq \norm{f(\bar{x}) - f(\bar{u})}
    + \norm{f(\bar{u}) - f(u_k)} < \epsilon' + \epsilon'', \; \forall
    \epsilon', \epsilon'' < \epsilon
\]
and also that
\[
    \norm{\bar{x} - u_k} \leq \norm{\bar{x} - \bar{u}} + \norm{\bar{u}- u_k}
    \leq \varepsilon' + \varepsilon'' < \varepsilon, \; \forall
    \varepsilon', \varepsilon'' < \varepsilon.
\]
This implies that $\exists w_k \in \hat{\partial} f(u_k), \; f(u_k) \to
f(\bar{x}), u_k \to \bar{x}$ with $\norm{w_k} \to 0$, which means that $0 \in
\partial f(\bar{x})$, a clear contradiction.
Name those ``safe limits'' $\epsilon_*, \varepsilon_*$. Then, we know that
\[
    \mathrm{dist}_{\partial f(u)}(0) \geq \kappa, \; \forall u
    \in U(\epsilon_*) \cap \mathbb{B}(\bar{x}, \varepsilon_*),
\]
for some $\kappa > 0$, implying that $\phi(s) = \kappa^{-1} s$ satisfies the
KL property at $\bar{x}$ with $\phi(0) = 0, \phi' = \kappa^{-1} > 0$, defined
on a slice $U(\epsilon_*)$ in the neighbourhood $\mathbb{B}(\bar{x},
\varepsilon_*)$, since
\[
    \phi'(f(u) - f(\bar{x})) \mathrm{dist}_{\partial f(u)}(0) >
    \kappa^{-1} \kappa \geq 1.
\]

\section*{Problem 4}
Consider $\cZ = \set{\bar{z} \in \Rbb^n \mmid \lim_{k \in K \subset \Nbb} z_{k}
= \bar{z}}$, where $\set{z_k}$ is an arbitrary bounded sequence that satisfies
$\norm{z_k - z_{k+1}} \to 0$. We have to prove the following properties:
\begin{itemize}
\item nonemptiness: this follows trivially, since $\set{z_k}$ is a bounded
sequence and by the Bolzano-Weierstrass Theorem admits at least one convergent
subsequence.
\item compactness: $\cZ$ has to be a bounded set, since otherwise we would be
able to find a sequence $\set{\bar{z}_k}, \norm{\bar{z}_k} \to \infty$. This
would imply the existence of an unbounded subsequence $\set{z_{j_k}} \to
\bar{z}_k$, which contradicts the construction of $\cZ$.
Additionally, $\cZ$ is defined as the closure of all convergent subsequences of
$\set{z_k}$ with the aforementioned properties, hence itself closed.
\item connectedness: assume, for the sake of contradiction, that $\cZ$ was not
a connected set. This means that there exists a pair of open sets $U, V$ such
that $\cZ = U \cup V$ with $U \cap V = \emptyset$. This implies that
$\exists \epsilon: \inf_{u \in U, v \in V} \norm{u - v} > \epsilon$. However,
this means that there exist two subsequences of $\set{z_k}$,
\[
    u_{k} \to u, \; v_{n} \to v, \; \norm{u - v} > \epsilon.
\]

\end{itemize}
Finally, we prove that $d_{\cZ}(z_k) \to 0$. We write
\[
    d_{\cZ}(z_k) = \inf_{z \in \cZ} \norm{z - z_k}
\]

\section*{Problem 5}
\paragraph{Limiting Subdifferential.}
Define $f(u, v) = \abs{u} - \abs{v}$, which does not have a regular subgradient
at $(0, 0)$ since $-\abs{v}$ admits no regular subgradient at $0$. Therefore, we
must deal with sequences of approaching regular subgradients along paths where
$\regdiff f(u, v)$ exists and approaches a limit. Consider the following cases:
\begin{itemize}
\item $u, v \neq 0$: in this case, $f$ is differentiable with $\regdiff f (u,
v) = \grad f (u, v) = \sign(u) - \sign(v)$.
\item $u = 0, v \neq 0$: in this case, $\partial \abs{u}(0)= [-1, 1]$ by known
results about convex subdifferentials, so the limiting subdifferential will
depend on the approach along $(0, v)$. The sum rule gives us
\[
    \regdiff f(u, v) \subset \regdiff \abs{u} + \regdiff (-\abs{v}) =
    \begin{pmatrix} [-1, 1] \\ 0 \end{pmatrix} + \begin{pmatrix}
        0 \\ \regdiff (-\abs{v}) \end{pmatrix} =
    \begin{pmatrix}
        [-1, 1] \\ \sign(v)
    \end{pmatrix}
\]
Notice that the above inclusion is actually an equality, since we can plug in
any element from the RHS in the definition of $\regdiff f$ and verify the
reverse inclusion. We have that $(g_1, g_2)^\top \in \regdiff f(0, v)$ if
\begin{align*}
    f(0 + \lambda_1, v + \lambda_2) &=
        \abs{\lambda_1} - \abs{v + \lambda_2} \geq
        0 - \abs{v} + g_1 \lambda_1 + g_2 \lambda_2 + o\left(
        \bm{\lambda}\right).
\end{align*}
Notice that, for any $g_1 \in [-1, 1]$, we have that $\abs{\lambda_1} \geq g_1
\lambda_1$. Moreover, setting $g_2 = \sign(v) = \grad (-\abs{v})$ satisfies by
the local linearization property of the gradient:
\[
    -\abs{v + \lambda_2} \geq -\abs{v} + \sign(v) \lambda_2 + o(\lambda_2),
\]
therefore we conclude that $\regdiff f(0, v) = \pmx{[-1, 1] \\ \sign(v)}$.
\item $v = 0, u \neq 0$: in that case, we show that $-\abs{v}$ has no regular
subdifferential. If that existed, then we would have $g$ such that
\[
    -\abs{d} \geq -\abs{0} + g d + o(d) \Rightarrow \abs{d} + g d + o(d) \leq 0,
    \; \forall d \in \Rbb^n
\]
If $g = 0$, we get $\abs{d} + o(d) \leq 0$, which is impossible. If $g \neq 0$,
for $d = \lambda \sign(g), \lambda > 0$, we obtain the contradiction
\[
    \lambda (1 + \abs{g}) + o(\lambda) \leq 0 \Rightarrow
    \lim_{\lambda \to 0^+} \frac{\lambda (1 + \abs{g})}{\norm{\lambda}}
    + \cancelto{0}{\frac{o(\lambda)}{\norm{\lambda}}} \leq 0 \Rightarrow
    1 + \abs{g} \leq 0.
\]
\end{itemize}
Combining the above approaches, we conclude that
\[
    \partial f(u, v) = [-1, 1] \times \set{-1, 1}
\]
since for $u = 0, v \neq 0$ we can find approaching sequences $(0, v_k), (0,
w_{\ell}) \to (0, 0)$ with $v_k > 0$ and $w_{\ell} < 0$ which lead to sequences
of regular subdifferentials $\partial f(0, v_k) = [-1, 1] \times \set{1}, \;
\partial f(0, w_{\ell}) = [-1, 1] \times \set{-1}$.


\paragraph{Limiting Slope.}
For the limiting slope, we know that
\[
    \overline{\abs{\grad f}}(0, 0) = \liminf_{\substack{u \to (0, 0) \\ f(u)
    \to f(0, 0)}} \abs{\grad f}(u).
\]
For any point $u \neq (0, 0)$, we have $\abs{\grad f}(u) =
\limsup_{u \to 0} \frac{f(u) - f(0)}{\norm{u - 0}} = \limsup_{u \to 0}
\frac{\abs{u_1} - \abs{u_2}}{\norm{u}}$.

\section*{Problem 6}
Consider the unconstrained low-rank matrix factorization problem:
\[
    \inf\set{\frac{1}{2}\norm{A - X}_F^2, \; \rank(X) \leq r}
\]
The Eckhart-Young theorem suggests that the optimal low-rank approximation is
given by
\[
    X_* = U \mathrm{diag}(\sigma_1, \dots, \sigma_r, 0, \dots 0) V^\top,
\]
where $A = U \mathrm{diag}(\sigma_1, \dots, \sigma_n) V^\top$ is the singular
value
decomposition of $A$, where the singular values are ordered as $\sigma_1 \geq
\sigma_2 \geq \dots \geq \sigma_n$.
On the other hand, the alternating proximal gradient applied to
\[
    \Psi(X, Y) = \frac{1}{2} \norm{A - XY}_F^2
\]
yields the following updates:
\begin{algorithm}
    \caption{Alternating gradients for low-rank approximation}
    \begin{algorithmic}
        \Repeat
            \State $X_{k+1} \gets \prox{\lambda \delta_{\Rbb^{n \times r}}}{
                X_k - \frac{1}{\lambda} \grad_X \Psi(X_k, Y_k)}$
            \State $Y_{k+1} \gets \prox{\nu \delta_{\Rbb^{r \times n}}}{
                Y_k - \frac{1}{\nu} \grad_Y \Psi(X_{k+1}, Y_k)}$
        \Until{convergence}
    \end{algorithmic}
\end{algorithm}
However, $\prox{\lambda \delta_{\Hbb}}{x} = x$ when $\Hbb$ is the underlying
space, therefore the above reduces to alternating gradient descent steps.
We write
\[
    \grad_X \Psi(X, Y) = (XY - A) Y^\top, \; \grad_Y \Psi(X, Y) = X^\top (XY -
    A).
\]
Notice that the gradient is separately Lipschitz since we have
\begin{align*}
    \norm{\grad_{X} \Psi(X_1, Y) - \grad_X \Psi(X_2, Y)}_F &=
        \norm{X_1 Y Y^\top - X_2 Y Y^\top}_F \leq \opnorm{YY^\top}
        \norm{X_1 - X_2}_F = \sigma_{\max}^2(Y) \norm{X_1 - X_2}_F
\end{align*}
and similarly for $\grad_Y$. Setting $\lambda = \sigma_{\max}^2(Y_k), \nu =
\sigma_{\max}^2(X_{k+1})$, we obtain the convergence plots shown
in~\cref{fig:rankA-15,fig:rankA-30}, where all matrices are initialized as
Gaussian ensembles. The full code can be found in Listing~\ref{code:p6}.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.49 \textwidth}
    \centering
    \begin{tikzpicture}% table
    \begin{axis}[xlabel=$k$,ylabel={$\Psi(X_k, Y_k)$}, ymode=log,
                 legend pos=north east, width=\linewidth]
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_50_rank_15.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_100_rank_15.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_150_rank_15.csv};
    \legend{\scalebox{0.75}{$n = 50$}, \scalebox{0.75}{$n = 100$},
            \scalebox{0.75}{$n = 150$}};
    \end{axis}
    \end{tikzpicture}
    \caption{Error plots for $\rank(A) = 15$}
    \label{fig:rankA-15}
    \end{minipage}~
    \begin{minipage}{0.49 \textwidth}
    \centering
    \begin{tikzpicture}% table
    \begin{axis}[xlabel=$k$,ylabel={$\Psi(X_k, Y_k)$}, ymode=log,
                 legend pos=south west, width=\linewidth]
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_50_rank_30.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_100_rank_30.csv};
    \addplot+[no markers, thick] table[x expr=\coordindex+1, y index=0, col
    sep=comma] {prox_dim_150_rank_30.csv};
    \legend{\scalebox{0.75}{$n = 50$}, \scalebox{0.75}{$n = 100$},
            \scalebox{0.75}{$n = 150$}};
    \end{axis}
    \end{tikzpicture}
    \caption{Error plots for $\rank(A) = 30$}
    \label{fig:rankA-30}
    \end{minipage}
\end{figure}

We point out that the distance of the final iterate in instances where the rank
of the matrix $A$ is small compared to the ambient dimension $n$ is at least as
good as the SVD solution, for a moderate value of iterations. When we increase
the rank $r$ compared to $n$, the proximal gradient method needs more
iterations to converge to a solution. However, given enough iterations, it
always seems to produce a global minimizer since the final error is close to
$0$.

\begin{longlisting}
	\inputminted{python}{hw2_p6.py}
	\caption{Script for Problem 6}
	\label{code:p6}
\end{longlisting}

\end{document}
